{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0zuYQzwiO8Z",
        "colab_type": "text"
      },
      "source": [
        "# M2608.001300 Machine Learning<br> Assignment #5 Final Projects (Pytorch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri0oOICuC64e",
        "colab_type": "text"
      },
      "source": [
        "Copyright (C) Data Science & AI Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJfG1l3RC_c3",
        "colab_type": "text"
      },
      "source": [
        "**For understanding of this work, please carefully look at given PPT file.**\n",
        "\n",
        "Note: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9yv4oGGDbmJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "aa7e371c-17cd-459b-824b-c63c6fd661ef"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "from PIL import ImageFilter\n",
        "!pip3 install resnet\n",
        "#import resnet\n",
        "import torchvision.models as models\n",
        "#!ls '/content/drive/My Drive/Data'\n",
        "import cv2\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: resnet in /usr/local/lib/python3.6/dist-packages (0.1)\n",
            "Requirement already satisfied: keras>=2.0 in /usr/local/lib/python3.6/dist-packages (from resnet) (2.3.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0->resnet) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0->resnet) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0->resnet) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0->resnet) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0->resnet) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0->resnet) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0->resnet) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acvGcUAaEkxe",
        "colab_type": "text"
      },
      "source": [
        "Load datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcPk4u8qGZHB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "230302f7-2e04-46ef-ddcf-740875f7df9a"
      },
      "source": [
        "NUMBER = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "ALPHABET = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
        "NONE = ['NONE'] # label for empty space\n",
        "ALL_CHAR_SET = NUMBER + ALPHABET + NONE\n",
        "ALL_CHAR_SET_LEN = len(ALL_CHAR_SET)\n",
        "MAX_CAPTCHA = 7\n",
        "\n",
        "print(ALL_CHAR_SET.index('NONE'))\n",
        "\n",
        "def encode(a):\n",
        "    onehot = [0]*ALL_CHAR_SET_LEN\n",
        "    idx = ALL_CHAR_SET.index(a)\n",
        "    onehot[idx] += 1\n",
        "    return onehot\n",
        "\n",
        "# modified dataset class\n",
        "class Mydataset(Dataset):\n",
        "    def __init__(self, img_path, label_path, is_train=True, transform=None):\n",
        "        self.path = img_path\n",
        "        self.label_path = label_path\n",
        "        if is_train: \n",
        "            self.img = os.listdir(self.path)[:10000]\n",
        "            self.labels = open(self.label_path, 'r').read().split('\\n')[:-1][:10000]\n",
        "        else: \n",
        "            self.img = os.listdir(self.path)[:1000]\n",
        "            self.labels = open(self.label_path, 'r').read().split('\\n')[:-1][:1000]\n",
        "        self.img=sorted(self.img, key=lambda a:(len(a),int(a[:-4])))\n",
        "        self.transform = transform\n",
        "        self.max_length = MAX_CAPTCHA\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img[idx]\n",
        "        img = Image.open(f'{self.path}/{self.img[idx]}')\n",
        "        img = img.convert('L')\n",
        "        img=img.filter(ImageFilter.MinFilter(size=3))\n",
        "        label = self.labels[idx]\n",
        "        label_oh = []\n",
        "        # one-hot for each character\n",
        "        for i in range(self.max_length):\n",
        "            if i < len(label):\n",
        "                label_oh += encode(label[i])\n",
        "            else:\n",
        "                #label_oh += [0]*ALL_CHAR_SET_LEN\n",
        "                label_oh += encode('NONE')\n",
        "            \n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img, np.array(label_oh), label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.img)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize([160, 60]),\n",
        "    transforms.ToTensor(),\n",
        "##############################################################################\n",
        "#                          IMPLEMENT YOUR CODE                               #\n",
        "##############################################################################\n",
        "    #preprocessed above.\n",
        "##############################################################################\n",
        "#                          END OF YOUR CODE                                  #\n",
        "##############################################################################\n",
        "])\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EFtlQyubGJZb",
        "colab": {}
      },
      "source": [
        "\"\"\"Loading DATA\"\"\"\n",
        "# Change to your own data folder path!\n",
        "gPath = '/content/drive/My Drive/'\n",
        "\n",
        "train_ds = Mydataset(gPath+'Data/train/', gPath+'Data/train.txt',transform=transform)\n",
        "test_ds = Mydataset(gPath+'Data/test/', gPath+'Data/test.txt',False, transform)\n",
        "#print(train_ds.__len__())\n",
        "train_dl = DataLoader(train_ds, batch_size=8, num_workers=4) #128 no.\n",
        "test_dl = DataLoader(test_ds, batch_size=1, num_workers=4)\n",
        "\n",
        "#for i,j in enumerate(train_dl):\n",
        "#  arr_=np.squeeze(j[0][1])\n",
        "#  plt.imshow(arr_)\n",
        "#  plt.show()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka5SgX6VIWcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"To CUDA for local run\"\"\"\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#GPUID = '4' # define GPUID\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPUID)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP3zI-YADV-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "I=6\n",
        "vocab=37\n",
        "hidden=8\n",
        "batch=8\n",
        "rnn = nn.LSTMCell(vocab, hidden)\n",
        "input = torch.randn(I, batch, vocab)\n",
        "hx = torch.randn(batch, hidden)\n",
        "cx = torch.randn(batch, hidden)\n",
        "output = []\n",
        "for i in range(I):\n",
        "        hx, cx = rnn(input[i], (hx, cx))\n",
        "        output.append(hx)\n",
        "        #print(hx.size())\n",
        "#print(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJaHW3wSENjY",
        "colab_type": "text"
      },
      "source": [
        "Problem 1: Design LSTM model for catcha image recognition. (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rHEe3XmBFQHq",
        "colab": {}
      },
      "source": [
        "cnn_dim=259 #resnet18-512\n",
        "hidden_size=8\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, cnn_dim, hidden_size, vocab_size, num_layers=1):\n",
        "        super(LSTM, self).__init__()\n",
        "        \n",
        "        # define the properties\n",
        "        self.cnn_dim = cnn_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        # lstm cell\n",
        "        self.lstm_cell = nn.LSTMCell(input_size=self.vocab_size, hidden_size=hidden_size)\n",
        "    \n",
        "        # output fully connected layer\n",
        "        self.fc_in = nn.Linear(in_features=self.cnn_dim, out_features=self.vocab_size)\n",
        "        self.fc_out = nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size)\n",
        "    \n",
        "        # embedding layer\n",
        "        self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.vocab_size)\n",
        "    \n",
        "        # activations\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "    \n",
        "    def forward(self, features, captions): \n",
        "\n",
        "        batch_size = features.size(0)\n",
        "        cnn_dim = features.size(1)\n",
        "\n",
        "        hidden_state = torch.zeros((batch_size, self.hidden_size)).cuda()\n",
        "        cell_state = torch.zeros((batch_size, self.hidden_size)).cuda()\n",
        "    \n",
        "        # define the output tensor placeholder\n",
        "        outputs = torch.empty((batch_size, captions.size(1), self.vocab_size)).cuda()\n",
        "\n",
        "        # embed the captions\n",
        "        captions_embed = self.embed(captions)\n",
        "\n",
        "##############################################################################\n",
        "#                          IMPLEMENT YOUR CODE                               #\n",
        "##############################################################################\n",
        "        for i in range(features.size()):\n",
        "            hidden_state, cell_state = rnn(input[i], (hidden_state, cell_state))\n",
        "            outputs[i]=hidden_state\n",
        "##############################################################################\n",
        "#                          END OF YOUR CODE                                  #\n",
        "##############################################################################\n",
        "        return outputs #8 7 37\n",
        "\n",
        "lstm = LSTM(cnn_dim=cnn_dim, hidden_size=hidden_size, vocab_size=vocab_size)\n",
        "#for i,j in enumerate(train_dl):\n",
        "  #print(j[1][0])\n",
        "  #print(lstm.forward(j[0],j[1]).size()) #16 7 37\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM2vL2PTFeEt",
        "colab_type": "text"
      },
      "source": [
        "Problem 2: \n",
        "\n",
        "*   1.Connect CNN model to the designed LSTM model.\n",
        "*   2.Replace ResNet to your own CNN model from Assignment3.\n",
        "\n",
        "\n",
        "          \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwgpQ1aiFq2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "##############################################################################\n",
        "#                          IMPLEMENT YOUR CODE                               #\n",
        "##############################################################################\n",
        "\"\"\"ResNet\"\"\"\n",
        "\n",
        "\n",
        "class Inception(nn.Module):\n",
        "    def __init__(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):\n",
        "        super(Inception, self).__init__()\n",
        "        # 1x1 conv branch\n",
        "        self.b1 = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, n1x1, kernel_size=1),\n",
        "            nn.BatchNorm2d(n1x1),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "        # 1x1 conv -> 3x3 conv branch\n",
        "        self.b2 = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, n3x3red, kernel_size=1),\n",
        "            nn.BatchNorm2d(n3x3red),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n3x3),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "        # 1x1 conv -> 5x5 conv branch\n",
        "        self.b3 = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, n5x5red, kernel_size=1),\n",
        "            nn.BatchNorm2d(n5x5red),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(n5x5red, n5x5, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n5x5),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n5x5),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "        # 3x3 pool -> 1x1 conv branch\n",
        "        self.b4 = nn.Sequential(\n",
        "            nn.MaxPool2d(3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n",
        "            nn.BatchNorm2d(pool_planes),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y1 = self.b1(x)\n",
        "        y2 = self.b2(x)\n",
        "        y3 = self.b3(x)\n",
        "        y4 = self.b4(x)\n",
        "        return torch.cat([y1,y2,y3,y4], 1)\n",
        "class BetterNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BetterNet, self).__init__()\n",
        "        self.preprocess = nn.Sequential(\n",
        "          nn.Conv2d(1, 192, kernel_size=3, padding=1),\n",
        "          nn.BatchNorm2d(192),\n",
        "          nn.ReLU(True),\n",
        "        )\n",
        "        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n",
        "        self.a_1 = Inception(192,  64,  96, 128, 16, 32, 32)\n",
        "        self.b_1 = Inception(256, 128, 128, 192, 32, 96, 64)\n",
        "        self.a_2 = Inception(480, 192,  96, 208, 16,  48,  64)\n",
        "        self.b_2 = Inception(512, 160, 112, 224, 24,  64,  64)\n",
        "        self.c_2 = Inception(512, 128, 128, 256, 24,  64,  64)\n",
        "        self.d_2 = Inception(512, 112, 144, 288, 32,  64,  64)\n",
        "        self.e_2 = Inception(528, 256, 160, 320, 32, 128, 128)\n",
        "        self.a_3 = Inception(832, 256, 160, 320, 32, 128, 128)\n",
        "        self.b_3 = Inception(832, 384, 192, 384, 48, 128, 128)\n",
        "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
        "        self.linear = nn.Linear(270336, 259*8) #259\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.preprocess(x) # 8 192 160 60\n",
        "        out = self.a_1(out)\n",
        "        out = self.b_1(out)\n",
        "        out = self.maxpool(out)\n",
        "        out = self.a_2(out)\n",
        "        out = self.b_2(out)\n",
        "        out = self.c_2(out)\n",
        "        out = self.d_2(out)\n",
        "        out = self.e_2(out)\n",
        "        out = self.maxpool(out)\n",
        "        out = self.a_3(out)\n",
        "        out = self.b_3(out)\n",
        "        out = self.avgpool(out) # 8 1024 33 8\n",
        "        out = out.view(out.size(0), -1)  #8  270336\n",
        "        out = self.linear(out)\n",
        "        out = out.view(out.size(0), 8,  -1)\n",
        "        return out\n",
        "#CNN\n",
        "#betternet = models.resnet18(pretrained=False)\n",
        "betternet=BetterNet()\n",
        "#betternet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "#betternet.fc = nn.Linear(in_features=512, out_features=ALL_CHAR_SET_LEN*MAX_CAPTCHA, bias=True)\n",
        "betternet = betternet.to(device)\n",
        "##############################################################################\n",
        "#                          END OF YOUR CODE                                  #\n",
        "##############################################################################\n",
        "\n",
        "      \n",
        "# LSTM\n",
        "cnn_dim=259 #resnet18-512\n",
        "hidden_size=8\n",
        "vocab_size=37 #ALL_CHAR_SET_LEN\n",
        "lstm1 = nn.LSTM(cnn_dim,hidden_size,vocab_size)#LSTM(cnn_dim=cnn_dim, hidden_size=hidden_size, vocab_size=vocab_size)\n",
        "lstm1 = lstm1.to(device)\n",
        "# loss, optimizer\n",
        "##############################################################################\n",
        "#                          IMPLEMENT YOUR CODE                               #\n",
        "##############################################################################\n",
        "loss_func = nn.MultiLabelSoftMarginLoss()\n",
        "cnn_optim = torch.optim.Adam(betternet.parameters(), lr=0.001)\n",
        "\n",
        "##############################################################################\n",
        "#                          END OF YOUR CODE                                  #\n",
        "##############################################################################"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoeTIkXjHJIE",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSvn2ZXZa6kI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "e614ed91-9331-4bf3-9e10-ca515b447055"
      },
      "source": [
        "PATH = './better_net.pth'\n",
        "\n",
        "#with torch.no_grad():\n",
        "\n",
        "optimizer = optim.SGD(betternet.parameters(), lr=0.001, momentum=0.9) #is 0.001 too high?\n",
        "def train(net1, net2, trainloader, max_epoch, crit, opt, model_path='./cifar_net.pth'):\n",
        "\n",
        "    for epoch in range(max_epoch):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs  =  data[0]\n",
        "            labels = data[1]\n",
        "            #for i in range(0,8):\n",
        "            #  arr_=np.squeeze(inputs[i])\n",
        "            #  plt.imshow(arr_)  \n",
        "            #  plt.show()\n",
        "            #print(labels)\n",
        "            # Training on GPU\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs1 = net1(inputs)\n",
        "            print(outputs1.size())\n",
        "            loss1 = crit(outputs1, labels)\n",
        "            loss1.backward()\n",
        "            opt.step()\n",
        "\n",
        "            outputs2 = net2(outputs1)\n",
        "            loss2 = crit(outputs2, labels)\n",
        "            loss2.backward()\n",
        "            opt.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss1.item()+loss2.item()\n",
        "            if i % 100 == 99:    # print every 300 mini-batches\n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss / 100))\n",
        "                running_loss = 0.0\n",
        "\n",
        "    print('Finished Training')\n",
        "    torch.save(net1.state_dict(), model_path)\n",
        "    torch.save(net2.state_dict(), model_path)\n",
        "    print('Saved Trained Model')\n",
        "train(betternet,lstm1, train_dl, 5, loss_func, cnn_optim, PATH)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-391f86878a9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Saved Trained Model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbetternet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlstm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_optim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-391f86878a9e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net1, net2, trainloader, max_epoch, crit, opt, model_path)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0moutputs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mloss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-67f7cb7cff99>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 8 192 160 60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-67f7cb7cff99>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0my3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0my4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    344\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    345\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 346\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 11.17 GiB total capacity; 10.79 GiB already allocated; 35.81 MiB free; 10.83 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0uCexwRHsNz",
        "colab_type": "text"
      },
      "source": [
        "Problem3: Find hyper-parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibfVzKZeH1yC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "660eac64-4979-48b1-b77c-f4be42a96ff0"
      },
      "source": [
        "\"\"\"TRAINING\"\"\"\n",
        "\n",
        "\n",
        "print_interval = 10\n",
        "max_epoch = 10 #40,1000\n",
        "#print(train_dl.__len__())\n",
        "for epoch in range(max_epoch):\n",
        "    for step, i in enumerate(train_dl):\n",
        "        img, label_oh, label = i\n",
        "        img = Variable(img).cuda()\n",
        "        label_oh = Variable(label_oh.float()).cuda()\n",
        "        batch_size, _ = label_oh.shape #4*259\n",
        "        #pred, feature = betternet(img)\n",
        "        pred = betternet(img)\n",
        "        loss = loss_func(pred, label_oh)\n",
        "        cnn_optim.zero_grad()\n",
        "        loss.backward()\n",
        "        cnn_optim.step()###should be changed\n",
        "##############################################################################\n",
        "#                          IMPLEMENT YOUR CODE                               #\n",
        "##############################################################################\n",
        "\n",
        "##############################################################################\n",
        "#                          END OF YOUR CODE                                  #\n",
        "##############################################################################\n",
        "        if (step+1)%print_interval == 0:\n",
        "            print('epoch:', epoch+1, 'step:', step+1, 'loss:', loss.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1 step: 10 loss: 0.08188681304454803\n",
            "epoch: 1 step: 20 loss: 0.07848311960697174\n",
            "epoch: 1 step: 30 loss: 0.08017425239086151\n",
            "epoch: 1 step: 40 loss: 0.07288508117198944\n",
            "epoch: 1 step: 50 loss: 0.08147762715816498\n",
            "epoch: 1 step: 60 loss: 0.0734580010175705\n",
            "epoch: 1 step: 70 loss: 0.06640581786632538\n",
            "epoch: 1 step: 80 loss: 0.06938903778791428\n",
            "epoch: 1 step: 90 loss: 0.07761746644973755\n",
            "epoch: 1 step: 100 loss: 0.07943476736545563\n",
            "epoch: 1 step: 110 loss: 0.06714969873428345\n",
            "epoch: 1 step: 120 loss: 0.08524724841117859\n",
            "epoch: 1 step: 130 loss: 0.07974059134721756\n",
            "epoch: 1 step: 140 loss: 0.08511141687631607\n",
            "epoch: 1 step: 150 loss: 0.07282016426324844\n",
            "epoch: 1 step: 160 loss: 0.06584198027849197\n",
            "epoch: 1 step: 170 loss: 0.06360799074172974\n",
            "epoch: 1 step: 180 loss: 0.07153229415416718\n",
            "epoch: 1 step: 190 loss: 0.06407849490642548\n",
            "epoch: 1 step: 200 loss: 0.07519645243883133\n",
            "epoch: 1 step: 210 loss: 0.08252124488353729\n",
            "epoch: 1 step: 220 loss: 0.06278344243764877\n",
            "epoch: 1 step: 230 loss: 0.08114833384752274\n",
            "epoch: 1 step: 240 loss: 0.072255939245224\n",
            "epoch: 1 step: 250 loss: 0.07456988841295242\n",
            "epoch: 1 step: 260 loss: 0.0708981603384018\n",
            "epoch: 1 step: 270 loss: 0.0673549622297287\n",
            "epoch: 1 step: 280 loss: 0.07277404516935349\n",
            "epoch: 1 step: 290 loss: 0.08535249531269073\n",
            "epoch: 1 step: 300 loss: 0.06672153621912003\n",
            "epoch: 1 step: 310 loss: 0.07614444196224213\n",
            "epoch: 1 step: 320 loss: 0.07188378274440765\n",
            "epoch: 1 step: 330 loss: 0.07190470397472382\n",
            "epoch: 1 step: 340 loss: 0.07259118556976318\n",
            "epoch: 1 step: 350 loss: 0.07072766125202179\n",
            "epoch: 1 step: 360 loss: 0.05736842378973961\n",
            "epoch: 1 step: 370 loss: 0.07550890743732452\n",
            "epoch: 1 step: 380 loss: 0.0832357406616211\n",
            "epoch: 1 step: 390 loss: 0.07253846526145935\n",
            "epoch: 1 step: 400 loss: 0.08690661191940308\n",
            "epoch: 1 step: 410 loss: 0.07199558615684509\n",
            "epoch: 1 step: 420 loss: 0.07375605404376984\n",
            "epoch: 1 step: 430 loss: 0.0690113753080368\n",
            "epoch: 1 step: 440 loss: 0.07263481616973877\n",
            "epoch: 1 step: 450 loss: 0.07137446105480194\n",
            "epoch: 1 step: 460 loss: 0.06376540660858154\n",
            "epoch: 1 step: 470 loss: 0.06699777394533157\n",
            "epoch: 1 step: 480 loss: 0.08308203518390656\n",
            "epoch: 1 step: 490 loss: 0.05753549188375473\n",
            "epoch: 1 step: 500 loss: 0.0746404230594635\n",
            "epoch: 1 step: 510 loss: 0.05968981236219406\n",
            "epoch: 1 step: 520 loss: 0.07032597064971924\n",
            "epoch: 1 step: 530 loss: 0.06488355249166489\n",
            "epoch: 1 step: 540 loss: 0.06727416813373566\n",
            "epoch: 1 step: 550 loss: 0.05631900206208229\n",
            "epoch: 1 step: 560 loss: 0.07112158834934235\n",
            "epoch: 1 step: 570 loss: 0.06362731754779816\n",
            "epoch: 1 step: 580 loss: 0.06184809282422066\n",
            "epoch: 1 step: 590 loss: 0.06218528747558594\n",
            "epoch: 1 step: 600 loss: 0.05794523283839226\n",
            "epoch: 1 step: 610 loss: 0.07013314962387085\n",
            "epoch: 1 step: 620 loss: 0.069547638297081\n",
            "epoch: 1 step: 630 loss: 0.08650268614292145\n",
            "epoch: 1 step: 640 loss: 0.0648997575044632\n",
            "epoch: 1 step: 650 loss: 0.06954502314329147\n",
            "epoch: 1 step: 660 loss: 0.06729421019554138\n",
            "epoch: 1 step: 670 loss: 0.06713993847370148\n",
            "epoch: 1 step: 680 loss: 0.08054690062999725\n",
            "epoch: 1 step: 690 loss: 0.07302837818861008\n",
            "epoch: 1 step: 700 loss: 0.07355600595474243\n",
            "epoch: 1 step: 710 loss: 0.06768572330474854\n",
            "epoch: 1 step: 720 loss: 0.05936605483293533\n",
            "epoch: 1 step: 730 loss: 0.07006549090147018\n",
            "epoch: 1 step: 740 loss: 0.07501129060983658\n",
            "epoch: 1 step: 750 loss: 0.06524962186813354\n",
            "epoch: 1 step: 760 loss: 0.08173654228448868\n",
            "epoch: 1 step: 770 loss: 0.07880672812461853\n",
            "epoch: 1 step: 780 loss: 0.06409817188978195\n",
            "epoch: 1 step: 790 loss: 0.070793516933918\n",
            "epoch: 1 step: 800 loss: 0.081453338265419\n",
            "epoch: 1 step: 810 loss: 0.07699693739414215\n",
            "epoch: 1 step: 820 loss: 0.06219860166311264\n",
            "epoch: 1 step: 830 loss: 0.06729099899530411\n",
            "epoch: 1 step: 840 loss: 0.07568921893835068\n",
            "epoch: 1 step: 850 loss: 0.07792757451534271\n",
            "epoch: 1 step: 860 loss: 0.057464007288217545\n",
            "epoch: 1 step: 870 loss: 0.08289838582277298\n",
            "epoch: 1 step: 880 loss: 0.07943277806043625\n",
            "epoch: 1 step: 890 loss: 0.06997636705636978\n",
            "epoch: 1 step: 900 loss: 0.05765477940440178\n",
            "epoch: 1 step: 910 loss: 0.07233420014381409\n",
            "epoch: 1 step: 920 loss: 0.06422215700149536\n",
            "epoch: 1 step: 930 loss: 0.06989142298698425\n",
            "epoch: 1 step: 940 loss: 0.057845622301101685\n",
            "epoch: 1 step: 950 loss: 0.07086373120546341\n",
            "epoch: 1 step: 960 loss: 0.0693640261888504\n",
            "epoch: 1 step: 970 loss: 0.07426737248897552\n",
            "epoch: 1 step: 980 loss: 0.07958580553531647\n",
            "epoch: 1 step: 990 loss: 0.08093474060297012\n",
            "epoch: 1 step: 1000 loss: 0.05990353226661682\n",
            "epoch: 1 step: 1010 loss: 0.08061552047729492\n",
            "epoch: 1 step: 1020 loss: 0.07424631714820862\n",
            "epoch: 1 step: 1030 loss: 0.07650652527809143\n",
            "epoch: 1 step: 1040 loss: 0.07476094365119934\n",
            "epoch: 1 step: 1050 loss: 0.06924370676279068\n",
            "epoch: 1 step: 1060 loss: 0.06855738162994385\n",
            "epoch: 1 step: 1070 loss: 0.07536321878433228\n",
            "epoch: 1 step: 1080 loss: 0.06212037056684494\n",
            "epoch: 1 step: 1090 loss: 0.061728425323963165\n",
            "epoch: 1 step: 1100 loss: 0.06527577340602875\n",
            "epoch: 1 step: 1110 loss: 0.06322365999221802\n",
            "epoch: 1 step: 1120 loss: 0.0740726962685585\n",
            "epoch: 1 step: 1130 loss: 0.06996557861566544\n",
            "epoch: 1 step: 1140 loss: 0.059177447110414505\n",
            "epoch: 1 step: 1150 loss: 0.07308972626924515\n",
            "epoch: 1 step: 1160 loss: 0.0595991387963295\n",
            "epoch: 1 step: 1170 loss: 0.06614410877227783\n",
            "epoch: 1 step: 1180 loss: 0.0649547129869461\n",
            "epoch: 1 step: 1190 loss: 0.07131858170032501\n",
            "epoch: 1 step: 1200 loss: 0.058778177946805954\n",
            "epoch: 1 step: 1210 loss: 0.06867791712284088\n",
            "epoch: 1 step: 1220 loss: 0.0684274211525917\n",
            "epoch: 1 step: 1230 loss: 0.07826437056064606\n",
            "epoch: 1 step: 1240 loss: 0.06272967159748077\n",
            "epoch: 1 step: 1250 loss: 0.05826368182897568\n",
            "epoch: 2 step: 10 loss: 0.07023226469755173\n",
            "epoch: 2 step: 20 loss: 0.06470537930727005\n",
            "epoch: 2 step: 30 loss: 0.06988488882780075\n",
            "epoch: 2 step: 40 loss: 0.06416521966457367\n",
            "epoch: 2 step: 50 loss: 0.07558203488588333\n",
            "epoch: 2 step: 60 loss: 0.06704982370138168\n",
            "epoch: 2 step: 70 loss: 0.05568869411945343\n",
            "epoch: 2 step: 80 loss: 0.061970055103302\n",
            "epoch: 2 step: 90 loss: 0.07252289354801178\n",
            "epoch: 2 step: 100 loss: 0.07199569046497345\n",
            "epoch: 2 step: 110 loss: 0.06083445996046066\n",
            "epoch: 2 step: 120 loss: 0.0805879756808281\n",
            "epoch: 2 step: 130 loss: 0.07417229562997818\n",
            "epoch: 2 step: 140 loss: 0.07949091494083405\n",
            "epoch: 2 step: 150 loss: 0.06627587974071503\n",
            "epoch: 2 step: 160 loss: 0.06224527955055237\n",
            "epoch: 2 step: 170 loss: 0.057959169149398804\n",
            "epoch: 2 step: 180 loss: 0.0657997578382492\n",
            "epoch: 2 step: 190 loss: 0.05918392911553383\n",
            "epoch: 2 step: 200 loss: 0.06746252626180649\n",
            "epoch: 2 step: 210 loss: 0.07272589206695557\n",
            "epoch: 2 step: 220 loss: 0.06132623180747032\n",
            "epoch: 2 step: 230 loss: 0.07669730484485626\n",
            "epoch: 2 step: 240 loss: 0.06725949048995972\n",
            "epoch: 2 step: 250 loss: 0.07058604806661606\n",
            "epoch: 2 step: 260 loss: 0.06766606122255325\n",
            "epoch: 2 step: 270 loss: 0.06243511289358139\n",
            "epoch: 2 step: 280 loss: 0.06632828712463379\n",
            "epoch: 2 step: 290 loss: 0.08668900281190872\n",
            "epoch: 2 step: 300 loss: 0.062413111329078674\n",
            "epoch: 2 step: 310 loss: 0.07372283935546875\n",
            "epoch: 2 step: 320 loss: 0.06825627386569977\n",
            "epoch: 2 step: 330 loss: 0.07041065394878387\n",
            "epoch: 2 step: 340 loss: 0.06556323915719986\n",
            "epoch: 2 step: 350 loss: 0.06995469331741333\n",
            "epoch: 2 step: 360 loss: 0.05163317173719406\n",
            "epoch: 2 step: 370 loss: 0.07196252048015594\n",
            "epoch: 2 step: 380 loss: 0.07492683827877045\n",
            "epoch: 2 step: 390 loss: 0.06876217573881149\n",
            "epoch: 2 step: 400 loss: 0.08312247693538666\n",
            "epoch: 2 step: 410 loss: 0.0692116767168045\n",
            "epoch: 2 step: 420 loss: 0.07103589177131653\n",
            "epoch: 2 step: 430 loss: 0.0654510110616684\n",
            "epoch: 2 step: 440 loss: 0.07015134394168854\n",
            "epoch: 2 step: 450 loss: 0.06764743477106094\n",
            "epoch: 2 step: 460 loss: 0.06069295480847359\n",
            "epoch: 2 step: 470 loss: 0.0666113793849945\n",
            "epoch: 2 step: 480 loss: 0.08042792230844498\n",
            "epoch: 2 step: 490 loss: 0.054371628910303116\n",
            "epoch: 2 step: 500 loss: 0.07037506997585297\n",
            "epoch: 2 step: 510 loss: 0.05606425926089287\n",
            "epoch: 2 step: 520 loss: 0.06895805895328522\n",
            "epoch: 2 step: 530 loss: 0.06288978457450867\n",
            "epoch: 2 step: 540 loss: 0.06690331548452377\n",
            "epoch: 2 step: 550 loss: 0.05481736734509468\n",
            "epoch: 2 step: 560 loss: 0.07103633880615234\n",
            "epoch: 2 step: 570 loss: 0.05936739593744278\n",
            "epoch: 2 step: 580 loss: 0.05693267285823822\n",
            "epoch: 2 step: 590 loss: 0.060262687504291534\n",
            "epoch: 2 step: 600 loss: 0.05534294620156288\n",
            "epoch: 2 step: 610 loss: 0.0672282800078392\n",
            "epoch: 2 step: 620 loss: 0.06617065519094467\n",
            "epoch: 2 step: 630 loss: 0.08725642412900925\n",
            "epoch: 2 step: 640 loss: 0.0612579807639122\n",
            "epoch: 2 step: 650 loss: 0.06653070449829102\n",
            "epoch: 2 step: 660 loss: 0.0650426596403122\n",
            "epoch: 2 step: 670 loss: 0.061717867851257324\n",
            "epoch: 2 step: 680 loss: 0.07629281282424927\n",
            "epoch: 2 step: 690 loss: 0.07339630275964737\n",
            "epoch: 2 step: 700 loss: 0.071539506316185\n",
            "epoch: 2 step: 710 loss: 0.06643320620059967\n",
            "epoch: 2 step: 720 loss: 0.055135779082775116\n",
            "epoch: 2 step: 730 loss: 0.06440244615077972\n",
            "epoch: 2 step: 740 loss: 0.07434676587581635\n",
            "epoch: 2 step: 750 loss: 0.06254824995994568\n",
            "epoch: 2 step: 760 loss: 0.0766068771481514\n",
            "epoch: 2 step: 770 loss: 0.07929021120071411\n",
            "epoch: 2 step: 780 loss: 0.06104098632931709\n",
            "epoch: 2 step: 790 loss: 0.06909874081611633\n",
            "epoch: 2 step: 800 loss: 0.07971911132335663\n",
            "epoch: 2 step: 810 loss: 0.0740460455417633\n",
            "epoch: 2 step: 820 loss: 0.05378788709640503\n",
            "epoch: 2 step: 830 loss: 0.06379595398902893\n",
            "epoch: 2 step: 840 loss: 0.07018503546714783\n",
            "epoch: 2 step: 850 loss: 0.07529474794864655\n",
            "epoch: 2 step: 860 loss: 0.054277557879686356\n",
            "epoch: 2 step: 870 loss: 0.0795731246471405\n",
            "epoch: 2 step: 880 loss: 0.07385760545730591\n",
            "epoch: 2 step: 890 loss: 0.06567078083753586\n",
            "epoch: 2 step: 900 loss: 0.05590316280722618\n",
            "epoch: 2 step: 910 loss: 0.07423573732376099\n",
            "epoch: 2 step: 920 loss: 0.06152565777301788\n",
            "epoch: 2 step: 930 loss: 0.06762301176786423\n",
            "epoch: 2 step: 940 loss: 0.05485562980175018\n",
            "epoch: 2 step: 950 loss: 0.06805410981178284\n",
            "epoch: 2 step: 960 loss: 0.06699763238430023\n",
            "epoch: 2 step: 970 loss: 0.07092563807964325\n",
            "epoch: 2 step: 980 loss: 0.07804065942764282\n",
            "epoch: 2 step: 990 loss: 0.07823674380779266\n",
            "epoch: 2 step: 1000 loss: 0.058652330189943314\n",
            "epoch: 2 step: 1010 loss: 0.07531006634235382\n",
            "epoch: 2 step: 1020 loss: 0.07133033871650696\n",
            "epoch: 2 step: 1030 loss: 0.07512542605400085\n",
            "epoch: 2 step: 1040 loss: 0.0724993348121643\n",
            "epoch: 2 step: 1050 loss: 0.06667309999465942\n",
            "epoch: 2 step: 1060 loss: 0.06660620868206024\n",
            "epoch: 2 step: 1070 loss: 0.07568423449993134\n",
            "epoch: 2 step: 1080 loss: 0.061714448034763336\n",
            "epoch: 2 step: 1090 loss: 0.05975314974784851\n",
            "epoch: 2 step: 1100 loss: 0.06448456645011902\n",
            "epoch: 2 step: 1110 loss: 0.06125649809837341\n",
            "epoch: 2 step: 1120 loss: 0.06931394338607788\n",
            "epoch: 2 step: 1130 loss: 0.06849683821201324\n",
            "epoch: 2 step: 1140 loss: 0.05981298163533211\n",
            "epoch: 2 step: 1150 loss: 0.0702952891588211\n",
            "epoch: 2 step: 1160 loss: 0.05713467299938202\n",
            "epoch: 2 step: 1170 loss: 0.06214626878499985\n",
            "epoch: 2 step: 1180 loss: 0.058897964656353\n",
            "epoch: 2 step: 1190 loss: 0.07260366529226303\n",
            "epoch: 2 step: 1200 loss: 0.05602262169122696\n",
            "epoch: 2 step: 1210 loss: 0.0672457218170166\n",
            "epoch: 2 step: 1220 loss: 0.06712106615304947\n",
            "epoch: 2 step: 1230 loss: 0.07903178036212921\n",
            "epoch: 2 step: 1240 loss: 0.0632689967751503\n",
            "epoch: 2 step: 1250 loss: 0.0577717199921608\n",
            "epoch: 3 step: 10 loss: 0.06868615746498108\n",
            "epoch: 3 step: 20 loss: 0.06360679864883423\n",
            "epoch: 3 step: 30 loss: 0.0680583268404007\n",
            "epoch: 3 step: 40 loss: 0.06331327557563782\n",
            "epoch: 3 step: 50 loss: 0.07422000169754028\n",
            "epoch: 3 step: 60 loss: 0.06601117551326752\n",
            "epoch: 3 step: 70 loss: 0.05445994436740875\n",
            "epoch: 3 step: 80 loss: 0.06255409866571426\n",
            "epoch: 3 step: 90 loss: 0.07231014966964722\n",
            "epoch: 3 step: 100 loss: 0.0716288834810257\n",
            "epoch: 3 step: 110 loss: 0.057409923523664474\n",
            "epoch: 3 step: 120 loss: 0.07937701046466827\n",
            "epoch: 3 step: 130 loss: 0.07279174029827118\n",
            "epoch: 3 step: 140 loss: 0.07591143250465393\n",
            "epoch: 3 step: 150 loss: 0.06424371153116226\n",
            "epoch: 3 step: 160 loss: 0.05915965139865875\n",
            "epoch: 3 step: 170 loss: 0.05719635635614395\n",
            "epoch: 3 step: 180 loss: 0.06297498941421509\n",
            "epoch: 3 step: 190 loss: 0.05925896018743515\n",
            "epoch: 3 step: 200 loss: 0.06617265939712524\n",
            "epoch: 3 step: 210 loss: 0.07119756937026978\n",
            "epoch: 3 step: 220 loss: 0.06158844754099846\n",
            "epoch: 3 step: 230 loss: 0.073344387114048\n",
            "epoch: 3 step: 240 loss: 0.06580308824777603\n",
            "epoch: 3 step: 250 loss: 0.0661165714263916\n",
            "epoch: 3 step: 260 loss: 0.06334259361028671\n",
            "epoch: 3 step: 270 loss: 0.06181555613875389\n",
            "epoch: 3 step: 280 loss: 0.06338100135326385\n",
            "epoch: 3 step: 290 loss: 0.08825263381004333\n",
            "epoch: 3 step: 300 loss: 0.061287377029657364\n",
            "epoch: 3 step: 310 loss: 0.07150942087173462\n",
            "epoch: 3 step: 320 loss: 0.06584326177835464\n",
            "epoch: 3 step: 330 loss: 0.0682181864976883\n",
            "epoch: 3 step: 340 loss: 0.06053284555673599\n",
            "epoch: 3 step: 350 loss: 0.06726088374853134\n",
            "epoch: 3 step: 360 loss: 0.05084387585520744\n",
            "epoch: 3 step: 370 loss: 0.07051610946655273\n",
            "epoch: 3 step: 380 loss: 0.07234113663434982\n",
            "epoch: 3 step: 390 loss: 0.06686876714229584\n",
            "epoch: 3 step: 400 loss: 0.08169408142566681\n",
            "epoch: 3 step: 410 loss: 0.06767122447490692\n",
            "epoch: 3 step: 420 loss: 0.06958209723234177\n",
            "epoch: 3 step: 430 loss: 0.06518834829330444\n",
            "epoch: 3 step: 440 loss: 0.06931104511022568\n",
            "epoch: 3 step: 450 loss: 0.06576503068208694\n",
            "epoch: 3 step: 460 loss: 0.059324003756046295\n",
            "epoch: 3 step: 470 loss: 0.06581404060125351\n",
            "epoch: 3 step: 480 loss: 0.07888877391815186\n",
            "epoch: 3 step: 490 loss: 0.05324652045965195\n",
            "epoch: 3 step: 500 loss: 0.0678219348192215\n",
            "epoch: 3 step: 510 loss: 0.0542326346039772\n",
            "epoch: 3 step: 520 loss: 0.0673687607049942\n",
            "epoch: 3 step: 530 loss: 0.061016637831926346\n",
            "epoch: 3 step: 540 loss: 0.06422032415866852\n",
            "epoch: 3 step: 550 loss: 0.05163507163524628\n",
            "epoch: 3 step: 560 loss: 0.07014226913452148\n",
            "epoch: 3 step: 570 loss: 0.05667822062969208\n",
            "epoch: 3 step: 580 loss: 0.056350771337747574\n",
            "epoch: 3 step: 590 loss: 0.058171346783638\n",
            "epoch: 3 step: 600 loss: 0.056602030992507935\n",
            "epoch: 3 step: 610 loss: 0.06777340173721313\n",
            "epoch: 3 step: 620 loss: 0.06545527279376984\n",
            "epoch: 3 step: 630 loss: 0.08512501418590546\n",
            "epoch: 3 step: 640 loss: 0.05967385694384575\n",
            "epoch: 3 step: 650 loss: 0.06681857258081436\n",
            "epoch: 3 step: 660 loss: 0.0632353350520134\n",
            "epoch: 3 step: 670 loss: 0.060683656483888626\n",
            "epoch: 3 step: 680 loss: 0.07357505708932877\n",
            "epoch: 3 step: 690 loss: 0.07071399688720703\n",
            "epoch: 3 step: 700 loss: 0.06865596026182175\n",
            "epoch: 3 step: 710 loss: 0.06529009342193604\n",
            "epoch: 3 step: 720 loss: 0.0522022470831871\n",
            "epoch: 3 step: 730 loss: 0.06268006563186646\n",
            "epoch: 3 step: 740 loss: 0.07231323421001434\n",
            "epoch: 3 step: 750 loss: 0.06176842004060745\n",
            "epoch: 3 step: 760 loss: 0.07528088986873627\n",
            "epoch: 3 step: 770 loss: 0.07747291773557663\n",
            "epoch: 3 step: 780 loss: 0.06131546571850777\n",
            "epoch: 3 step: 790 loss: 0.06756454706192017\n",
            "epoch: 3 step: 800 loss: 0.07958526909351349\n",
            "epoch: 3 step: 810 loss: 0.07283009588718414\n",
            "epoch: 3 step: 820 loss: 0.051588840782642365\n",
            "epoch: 3 step: 830 loss: 0.06278915703296661\n",
            "epoch: 3 step: 840 loss: 0.06880003213882446\n",
            "epoch: 3 step: 850 loss: 0.07235249131917953\n",
            "epoch: 3 step: 860 loss: 0.050513770431280136\n",
            "epoch: 3 step: 870 loss: 0.07813157141208649\n",
            "epoch: 3 step: 880 loss: 0.07342524826526642\n",
            "epoch: 3 step: 890 loss: 0.06392472982406616\n",
            "epoch: 3 step: 900 loss: 0.05271358788013458\n",
            "epoch: 3 step: 910 loss: 0.06923551112413406\n",
            "epoch: 3 step: 920 loss: 0.05988121032714844\n",
            "epoch: 3 step: 930 loss: 0.06525559723377228\n",
            "epoch: 3 step: 940 loss: 0.05337044969201088\n",
            "epoch: 3 step: 950 loss: 0.06682407110929489\n",
            "epoch: 3 step: 960 loss: 0.06556789577007294\n",
            "epoch: 3 step: 970 loss: 0.07231754064559937\n",
            "epoch: 3 step: 980 loss: 0.07678846269845963\n",
            "epoch: 3 step: 990 loss: 0.07584936916828156\n",
            "epoch: 3 step: 1000 loss: 0.058153800666332245\n",
            "epoch: 3 step: 1010 loss: 0.07243715971708298\n",
            "epoch: 3 step: 1020 loss: 0.06911429762840271\n",
            "epoch: 3 step: 1030 loss: 0.07409868389368057\n",
            "epoch: 3 step: 1040 loss: 0.07095026969909668\n",
            "epoch: 3 step: 1050 loss: 0.06260237097740173\n",
            "epoch: 3 step: 1060 loss: 0.06490964442491531\n",
            "epoch: 3 step: 1070 loss: 0.07558111846446991\n",
            "epoch: 3 step: 1080 loss: 0.059077341109514236\n",
            "epoch: 3 step: 1090 loss: 0.05667172372341156\n",
            "epoch: 3 step: 1100 loss: 0.06478527933359146\n",
            "epoch: 3 step: 1110 loss: 0.05944250524044037\n",
            "epoch: 3 step: 1120 loss: 0.0688110738992691\n",
            "epoch: 3 step: 1130 loss: 0.06616075336933136\n",
            "epoch: 3 step: 1140 loss: 0.059117577970027924\n",
            "epoch: 3 step: 1150 loss: 0.06929127871990204\n",
            "epoch: 3 step: 1160 loss: 0.05571237951517105\n",
            "epoch: 3 step: 1170 loss: 0.05992474779486656\n",
            "epoch: 3 step: 1180 loss: 0.057542331516742706\n",
            "epoch: 3 step: 1190 loss: 0.0687464028596878\n",
            "epoch: 3 step: 1200 loss: 0.054543834179639816\n",
            "epoch: 3 step: 1210 loss: 0.06592902541160583\n",
            "epoch: 3 step: 1220 loss: 0.0663810670375824\n",
            "epoch: 3 step: 1230 loss: 0.076552614569664\n",
            "epoch: 3 step: 1240 loss: 0.05876172333955765\n",
            "epoch: 3 step: 1250 loss: 0.05703573301434517\n",
            "epoch: 4 step: 10 loss: 0.06827200949192047\n",
            "epoch: 4 step: 20 loss: 0.06117923557758331\n",
            "epoch: 4 step: 30 loss: 0.066404327750206\n",
            "epoch: 4 step: 40 loss: 0.06080634891986847\n",
            "epoch: 4 step: 50 loss: 0.07326309382915497\n",
            "epoch: 4 step: 60 loss: 0.06334269046783447\n",
            "epoch: 4 step: 70 loss: 0.05668850988149643\n",
            "epoch: 4 step: 80 loss: 0.06145167350769043\n",
            "epoch: 4 step: 90 loss: 0.0725318044424057\n",
            "epoch: 4 step: 100 loss: 0.07252994179725647\n",
            "epoch: 4 step: 110 loss: 0.058602362871170044\n",
            "epoch: 4 step: 120 loss: 0.07751594483852386\n",
            "epoch: 4 step: 130 loss: 0.07119818031787872\n",
            "epoch: 4 step: 140 loss: 0.07428623735904694\n",
            "epoch: 4 step: 150 loss: 0.06272883713245392\n",
            "epoch: 4 step: 160 loss: 0.055328719317913055\n",
            "epoch: 4 step: 170 loss: 0.053193073719739914\n",
            "epoch: 4 step: 180 loss: 0.059980712831020355\n",
            "epoch: 4 step: 190 loss: 0.05693081393837929\n",
            "epoch: 4 step: 200 loss: 0.064899742603302\n",
            "epoch: 4 step: 210 loss: 0.07202474772930145\n",
            "epoch: 4 step: 220 loss: 0.06077026575803757\n",
            "epoch: 4 step: 230 loss: 0.07220505177974701\n",
            "epoch: 4 step: 240 loss: 0.06416136026382446\n",
            "epoch: 4 step: 250 loss: 0.0629509836435318\n",
            "epoch: 4 step: 260 loss: 0.06050950288772583\n",
            "epoch: 4 step: 270 loss: 0.059210650622844696\n",
            "epoch: 4 step: 280 loss: 0.06188586726784706\n",
            "epoch: 4 step: 290 loss: 0.08650065958499908\n",
            "epoch: 4 step: 300 loss: 0.060412339866161346\n",
            "epoch: 4 step: 310 loss: 0.06893307715654373\n",
            "epoch: 4 step: 320 loss: 0.06499946117401123\n",
            "epoch: 4 step: 330 loss: 0.06920966506004333\n",
            "epoch: 4 step: 340 loss: 0.05721055716276169\n",
            "epoch: 4 step: 350 loss: 0.06740352511405945\n",
            "epoch: 4 step: 360 loss: 0.04726822301745415\n",
            "epoch: 4 step: 370 loss: 0.06732042133808136\n",
            "epoch: 4 step: 380 loss: 0.07293891906738281\n",
            "epoch: 4 step: 390 loss: 0.06456279754638672\n",
            "epoch: 4 step: 400 loss: 0.08018994331359863\n",
            "epoch: 4 step: 410 loss: 0.06534905731678009\n",
            "epoch: 4 step: 420 loss: 0.06940402835607529\n",
            "epoch: 4 step: 430 loss: 0.062335304915905\n",
            "epoch: 4 step: 440 loss: 0.06834765523672104\n",
            "epoch: 4 step: 450 loss: 0.06565003097057343\n",
            "epoch: 4 step: 460 loss: 0.05781888589262962\n",
            "epoch: 4 step: 470 loss: 0.06473314017057419\n",
            "epoch: 4 step: 480 loss: 0.07998470962047577\n",
            "epoch: 4 step: 490 loss: 0.05021330714225769\n",
            "epoch: 4 step: 500 loss: 0.06521712243556976\n",
            "epoch: 4 step: 510 loss: 0.05051077902317047\n",
            "epoch: 4 step: 520 loss: 0.06657972931861877\n",
            "epoch: 4 step: 530 loss: 0.058048397302627563\n",
            "epoch: 4 step: 540 loss: 0.06261900067329407\n",
            "epoch: 4 step: 550 loss: 0.050323836505413055\n",
            "epoch: 4 step: 560 loss: 0.06770546734333038\n",
            "epoch: 4 step: 570 loss: 0.05383966863155365\n",
            "epoch: 4 step: 580 loss: 0.055283479392528534\n",
            "epoch: 4 step: 590 loss: 0.05363137274980545\n",
            "epoch: 4 step: 600 loss: 0.04995118826627731\n",
            "epoch: 4 step: 610 loss: 0.06713256239891052\n",
            "epoch: 4 step: 620 loss: 0.06344418972730637\n",
            "epoch: 4 step: 630 loss: 0.08398950099945068\n",
            "epoch: 4 step: 640 loss: 0.05733508616685867\n",
            "epoch: 4 step: 650 loss: 0.06357082724571228\n",
            "epoch: 4 step: 660 loss: 0.06046944856643677\n",
            "epoch: 4 step: 670 loss: 0.05756433308124542\n",
            "epoch: 4 step: 680 loss: 0.07157022505998611\n",
            "epoch: 4 step: 690 loss: 0.06742318719625473\n",
            "epoch: 4 step: 700 loss: 0.06546372175216675\n",
            "epoch: 4 step: 710 loss: 0.06260843575000763\n",
            "epoch: 4 step: 720 loss: 0.04979466646909714\n",
            "epoch: 4 step: 730 loss: 0.0590871199965477\n",
            "epoch: 4 step: 740 loss: 0.06948775053024292\n",
            "epoch: 4 step: 750 loss: 0.05824297294020653\n",
            "epoch: 4 step: 760 loss: 0.07232917845249176\n",
            "epoch: 4 step: 770 loss: 0.07286103814840317\n",
            "epoch: 4 step: 780 loss: 0.05606163293123245\n",
            "epoch: 4 step: 790 loss: 0.06447212398052216\n",
            "epoch: 4 step: 800 loss: 0.076714888215065\n",
            "epoch: 4 step: 810 loss: 0.07038624584674835\n",
            "epoch: 4 step: 820 loss: 0.05037947744131088\n",
            "epoch: 4 step: 830 loss: 0.059764355421066284\n",
            "epoch: 4 step: 840 loss: 0.06688999384641647\n",
            "epoch: 4 step: 850 loss: 0.07026896625757217\n",
            "epoch: 4 step: 860 loss: 0.046098869293928146\n",
            "epoch: 4 step: 870 loss: 0.07519732415676117\n",
            "epoch: 4 step: 880 loss: 0.06986474990844727\n",
            "epoch: 4 step: 890 loss: 0.058875374495983124\n",
            "epoch: 4 step: 900 loss: 0.049370113760232925\n",
            "epoch: 4 step: 910 loss: 0.06568549573421478\n",
            "epoch: 4 step: 920 loss: 0.05585748702287674\n",
            "epoch: 4 step: 930 loss: 0.058480046689510345\n",
            "epoch: 4 step: 940 loss: 0.052083589136600494\n",
            "epoch: 4 step: 950 loss: 0.06089074909687042\n",
            "epoch: 4 step: 960 loss: 0.06627854704856873\n",
            "epoch: 4 step: 970 loss: 0.06746700406074524\n",
            "epoch: 4 step: 980 loss: 0.0724995881319046\n",
            "epoch: 4 step: 990 loss: 0.07472303509712219\n",
            "epoch: 4 step: 1000 loss: 0.05794551968574524\n",
            "epoch: 4 step: 1010 loss: 0.07013043016195297\n",
            "epoch: 4 step: 1020 loss: 0.06726114451885223\n",
            "epoch: 4 step: 1030 loss: 0.07462351024150848\n",
            "epoch: 4 step: 1040 loss: 0.067166268825531\n",
            "epoch: 4 step: 1050 loss: 0.059866972267627716\n",
            "epoch: 4 step: 1060 loss: 0.060532182455062866\n",
            "epoch: 4 step: 1070 loss: 0.07545950263738632\n",
            "epoch: 4 step: 1080 loss: 0.059520527720451355\n",
            "epoch: 4 step: 1090 loss: 0.05262135714292526\n",
            "epoch: 4 step: 1100 loss: 0.06245362386107445\n",
            "epoch: 4 step: 1110 loss: 0.05542784929275513\n",
            "epoch: 4 step: 1120 loss: 0.06618981808423996\n",
            "epoch: 4 step: 1130 loss: 0.060984522104263306\n",
            "epoch: 4 step: 1140 loss: 0.055137112736701965\n",
            "epoch: 4 step: 1150 loss: 0.06836582720279694\n",
            "epoch: 4 step: 1160 loss: 0.05267941206693649\n",
            "epoch: 4 step: 1170 loss: 0.055613644421100616\n",
            "epoch: 4 step: 1180 loss: 0.05653546378016472\n",
            "epoch: 4 step: 1190 loss: 0.06440229713916779\n",
            "epoch: 4 step: 1200 loss: 0.05160673335194588\n",
            "epoch: 4 step: 1210 loss: 0.06259474158287048\n",
            "epoch: 4 step: 1220 loss: 0.06443092972040176\n",
            "epoch: 4 step: 1230 loss: 0.0767391175031662\n",
            "epoch: 4 step: 1240 loss: 0.057266347110271454\n",
            "epoch: 4 step: 1250 loss: 0.05402515083551407\n",
            "epoch: 5 step: 10 loss: 0.06643408536911011\n",
            "epoch: 5 step: 20 loss: 0.057932041585445404\n",
            "epoch: 5 step: 30 loss: 0.06209593266248703\n",
            "epoch: 5 step: 40 loss: 0.05672048032283783\n",
            "epoch: 5 step: 50 loss: 0.06896410882472992\n",
            "epoch: 5 step: 60 loss: 0.06141249090433121\n",
            "epoch: 5 step: 70 loss: 0.0460682287812233\n",
            "epoch: 5 step: 80 loss: 0.055704548954963684\n",
            "epoch: 5 step: 90 loss: 0.06908941268920898\n",
            "epoch: 5 step: 100 loss: 0.06781744956970215\n",
            "epoch: 5 step: 110 loss: 0.050257302820682526\n",
            "epoch: 5 step: 120 loss: 0.07281433045864105\n",
            "epoch: 5 step: 130 loss: 0.06839002668857574\n",
            "epoch: 5 step: 140 loss: 0.07205179333686829\n",
            "epoch: 5 step: 150 loss: 0.05818658322095871\n",
            "epoch: 5 step: 160 loss: 0.05417927727103233\n",
            "epoch: 5 step: 170 loss: 0.04730444401502609\n",
            "epoch: 5 step: 180 loss: 0.05549301207065582\n",
            "epoch: 5 step: 190 loss: 0.04991602152585983\n",
            "epoch: 5 step: 200 loss: 0.06038995832204819\n",
            "epoch: 5 step: 210 loss: 0.06682657450437546\n",
            "epoch: 5 step: 220 loss: 0.055627722293138504\n",
            "epoch: 5 step: 230 loss: 0.06889048963785172\n",
            "epoch: 5 step: 240 loss: 0.05996683984994888\n",
            "epoch: 5 step: 250 loss: 0.058111902326345444\n",
            "epoch: 5 step: 260 loss: 0.058011267334222794\n",
            "epoch: 5 step: 270 loss: 0.053829390555620193\n",
            "epoch: 5 step: 280 loss: 0.05801626294851303\n",
            "epoch: 5 step: 290 loss: 0.0760975256562233\n",
            "epoch: 5 step: 300 loss: 0.05397716909646988\n",
            "epoch: 5 step: 310 loss: 0.06220530346035957\n",
            "epoch: 5 step: 320 loss: 0.062220796942710876\n",
            "epoch: 5 step: 330 loss: 0.06609237194061279\n",
            "epoch: 5 step: 340 loss: 0.052404507994651794\n",
            "epoch: 5 step: 350 loss: 0.06096464768052101\n",
            "epoch: 5 step: 360 loss: 0.04179524630308151\n",
            "epoch: 5 step: 370 loss: 0.06222912669181824\n",
            "epoch: 5 step: 380 loss: 0.06820166110992432\n",
            "epoch: 5 step: 390 loss: 0.0605844110250473\n",
            "epoch: 5 step: 400 loss: 0.07465436309576035\n",
            "epoch: 5 step: 410 loss: 0.06042778864502907\n",
            "epoch: 5 step: 420 loss: 0.06214439868927002\n",
            "epoch: 5 step: 430 loss: 0.056858405470848083\n",
            "epoch: 5 step: 440 loss: 0.06219610571861267\n",
            "epoch: 5 step: 450 loss: 0.061980150640010834\n",
            "epoch: 5 step: 460 loss: 0.05437175929546356\n",
            "epoch: 5 step: 470 loss: 0.06098136305809021\n",
            "epoch: 5 step: 480 loss: 0.07254639267921448\n",
            "epoch: 5 step: 490 loss: 0.04494946077466011\n",
            "epoch: 5 step: 500 loss: 0.061678819358348846\n",
            "epoch: 5 step: 510 loss: 0.04542981833219528\n",
            "epoch: 5 step: 520 loss: 0.06278606504201889\n",
            "epoch: 5 step: 530 loss: 0.05206871032714844\n",
            "epoch: 5 step: 540 loss: 0.05895978957414627\n",
            "epoch: 5 step: 550 loss: 0.044469185173511505\n",
            "epoch: 5 step: 560 loss: 0.06225041300058365\n",
            "epoch: 5 step: 570 loss: 0.05054422840476036\n",
            "epoch: 5 step: 580 loss: 0.04871470481157303\n",
            "epoch: 5 step: 590 loss: 0.0475451834499836\n",
            "epoch: 5 step: 600 loss: 0.04523731768131256\n",
            "epoch: 5 step: 610 loss: 0.06259371340274811\n",
            "epoch: 5 step: 620 loss: 0.05933455377817154\n",
            "epoch: 5 step: 630 loss: 0.07724231481552124\n",
            "epoch: 5 step: 640 loss: 0.053785838186740875\n",
            "epoch: 5 step: 650 loss: 0.058941543102264404\n",
            "epoch: 5 step: 660 loss: 0.05641894042491913\n",
            "epoch: 5 step: 670 loss: 0.051858603954315186\n",
            "epoch: 5 step: 680 loss: 0.06505526602268219\n",
            "epoch: 5 step: 690 loss: 0.061191998422145844\n",
            "epoch: 5 step: 700 loss: 0.05873341113328934\n",
            "epoch: 5 step: 710 loss: 0.055555328726768494\n",
            "epoch: 5 step: 720 loss: 0.04598898068070412\n",
            "epoch: 5 step: 730 loss: 0.052657872438430786\n",
            "epoch: 5 step: 740 loss: 0.0655561313033104\n",
            "epoch: 5 step: 750 loss: 0.0541670024394989\n",
            "epoch: 5 step: 760 loss: 0.07011286169290543\n",
            "epoch: 5 step: 770 loss: 0.06626319885253906\n",
            "epoch: 5 step: 780 loss: 0.05078153684735298\n",
            "epoch: 5 step: 790 loss: 0.062039148062467575\n",
            "epoch: 5 step: 800 loss: 0.07148458063602448\n",
            "epoch: 5 step: 810 loss: 0.06322317570447922\n",
            "epoch: 5 step: 820 loss: 0.04479400813579559\n",
            "epoch: 5 step: 830 loss: 0.05314767360687256\n",
            "epoch: 5 step: 840 loss: 0.06310927122831345\n",
            "epoch: 5 step: 850 loss: 0.06426175683736801\n",
            "epoch: 5 step: 860 loss: 0.04259234666824341\n",
            "epoch: 5 step: 870 loss: 0.07060973346233368\n",
            "epoch: 5 step: 880 loss: 0.0633721649646759\n",
            "epoch: 5 step: 890 loss: 0.05521545186638832\n",
            "epoch: 5 step: 900 loss: 0.04376330226659775\n",
            "epoch: 5 step: 910 loss: 0.05972324311733246\n",
            "epoch: 5 step: 920 loss: 0.05176273733377457\n",
            "epoch: 5 step: 930 loss: 0.054982203990221024\n",
            "epoch: 5 step: 940 loss: 0.04371995851397514\n",
            "epoch: 5 step: 950 loss: 0.05757862702012062\n",
            "epoch: 5 step: 960 loss: 0.059731997549533844\n",
            "epoch: 5 step: 970 loss: 0.06238120049238205\n",
            "epoch: 5 step: 980 loss: 0.06738799810409546\n",
            "epoch: 5 step: 990 loss: 0.07069352269172668\n",
            "epoch: 5 step: 1000 loss: 0.05324909836053848\n",
            "epoch: 5 step: 1010 loss: 0.06692695617675781\n",
            "epoch: 5 step: 1020 loss: 0.06173183396458626\n",
            "epoch: 5 step: 1030 loss: 0.06838932633399963\n",
            "epoch: 5 step: 1040 loss: 0.06194780394434929\n",
            "epoch: 5 step: 1050 loss: 0.05476488918066025\n",
            "epoch: 5 step: 1060 loss: 0.056798726320266724\n",
            "epoch: 5 step: 1070 loss: 0.0675102025270462\n",
            "epoch: 5 step: 1080 loss: 0.05466296523809433\n",
            "epoch: 5 step: 1090 loss: 0.04657848924398422\n",
            "epoch: 5 step: 1100 loss: 0.058166272938251495\n",
            "epoch: 5 step: 1110 loss: 0.04850265383720398\n",
            "epoch: 5 step: 1120 loss: 0.05805622413754463\n",
            "epoch: 5 step: 1130 loss: 0.05612368881702423\n",
            "epoch: 5 step: 1140 loss: 0.0494360476732254\n",
            "epoch: 5 step: 1150 loss: 0.06216944754123688\n",
            "epoch: 5 step: 1160 loss: 0.0467405766248703\n",
            "epoch: 5 step: 1170 loss: 0.047951072454452515\n",
            "epoch: 5 step: 1180 loss: 0.04717147722840309\n",
            "epoch: 5 step: 1190 loss: 0.05876673758029938\n",
            "epoch: 5 step: 1200 loss: 0.044704824686050415\n",
            "epoch: 5 step: 1210 loss: 0.056332219392061234\n",
            "epoch: 5 step: 1220 loss: 0.0569738894701004\n",
            "epoch: 5 step: 1230 loss: 0.06801153719425201\n",
            "epoch: 5 step: 1240 loss: 0.05068453401327133\n",
            "epoch: 5 step: 1250 loss: 0.046051450073719025\n",
            "epoch: 6 step: 10 loss: 0.059915557503700256\n",
            "epoch: 6 step: 20 loss: 0.05059799551963806\n",
            "epoch: 6 step: 30 loss: 0.05602291226387024\n",
            "epoch: 6 step: 40 loss: 0.05108880624175072\n",
            "epoch: 6 step: 50 loss: 0.0627274140715599\n",
            "epoch: 6 step: 60 loss: 0.056303612887859344\n",
            "epoch: 6 step: 70 loss: 0.042820319533348083\n",
            "epoch: 6 step: 80 loss: 0.04976128786802292\n",
            "epoch: 6 step: 90 loss: 0.06204957515001297\n",
            "epoch: 6 step: 100 loss: 0.06164439022541046\n",
            "epoch: 6 step: 110 loss: 0.04276632145047188\n",
            "epoch: 6 step: 120 loss: 0.06744447350502014\n",
            "epoch: 6 step: 130 loss: 0.06619513034820557\n",
            "epoch: 6 step: 140 loss: 0.06701245903968811\n",
            "epoch: 6 step: 150 loss: 0.05072952061891556\n",
            "epoch: 6 step: 160 loss: 0.04858235642313957\n",
            "epoch: 6 step: 170 loss: 0.04107106104493141\n",
            "epoch: 6 step: 180 loss: 0.045258525758981705\n",
            "epoch: 6 step: 190 loss: 0.043372929096221924\n",
            "epoch: 6 step: 200 loss: 0.05225588008761406\n",
            "epoch: 6 step: 210 loss: 0.05641666799783707\n",
            "epoch: 6 step: 220 loss: 0.04911353439092636\n",
            "epoch: 6 step: 230 loss: 0.061697762459516525\n",
            "epoch: 6 step: 240 loss: 0.05116238445043564\n",
            "epoch: 6 step: 250 loss: 0.051551833748817444\n",
            "epoch: 6 step: 260 loss: 0.050910040736198425\n",
            "epoch: 6 step: 270 loss: 0.046838466078042984\n",
            "epoch: 6 step: 280 loss: 0.04922683537006378\n",
            "epoch: 6 step: 290 loss: 0.06528446823358536\n",
            "epoch: 6 step: 300 loss: 0.04999976605176926\n",
            "epoch: 6 step: 310 loss: 0.05690688639879227\n",
            "epoch: 6 step: 320 loss: 0.05493444204330444\n",
            "epoch: 6 step: 330 loss: 0.05952973663806915\n",
            "epoch: 6 step: 340 loss: 0.0471804104745388\n",
            "epoch: 6 step: 350 loss: 0.06365242600440979\n",
            "epoch: 6 step: 360 loss: 0.037662290036678314\n",
            "epoch: 6 step: 370 loss: 0.06102890148758888\n",
            "epoch: 6 step: 380 loss: 0.05869840830564499\n",
            "epoch: 6 step: 390 loss: 0.05745371803641319\n",
            "epoch: 6 step: 400 loss: 0.07405854016542435\n",
            "epoch: 6 step: 410 loss: 0.05161229148507118\n",
            "epoch: 6 step: 420 loss: 0.05508013069629669\n",
            "epoch: 6 step: 430 loss: 0.049630045890808105\n",
            "epoch: 6 step: 440 loss: 0.057171531021595\n",
            "epoch: 6 step: 450 loss: 0.05488264560699463\n",
            "epoch: 6 step: 460 loss: 0.04830276221036911\n",
            "epoch: 6 step: 470 loss: 0.05364952236413956\n",
            "epoch: 6 step: 480 loss: 0.06455712765455246\n",
            "epoch: 6 step: 490 loss: 0.039581261575222015\n",
            "epoch: 6 step: 500 loss: 0.056632231920957565\n",
            "epoch: 6 step: 510 loss: 0.0379335917532444\n",
            "epoch: 6 step: 520 loss: 0.05576125532388687\n",
            "epoch: 6 step: 530 loss: 0.04355810210108757\n",
            "epoch: 6 step: 540 loss: 0.050094857811927795\n",
            "epoch: 6 step: 550 loss: 0.03708196058869362\n",
            "epoch: 6 step: 560 loss: 0.05349188670516014\n",
            "epoch: 6 step: 570 loss: 0.04241044819355011\n",
            "epoch: 6 step: 580 loss: 0.04479546844959259\n",
            "epoch: 6 step: 590 loss: 0.04074396193027496\n",
            "epoch: 6 step: 600 loss: 0.03755081444978714\n",
            "epoch: 6 step: 610 loss: 0.05206531286239624\n",
            "epoch: 6 step: 620 loss: 0.05740785971283913\n",
            "epoch: 6 step: 630 loss: 0.06815646588802338\n",
            "epoch: 6 step: 640 loss: 0.04629581794142723\n",
            "epoch: 6 step: 650 loss: 0.051507677882909775\n",
            "epoch: 6 step: 660 loss: 0.05043897405266762\n",
            "epoch: 6 step: 670 loss: 0.04143013060092926\n",
            "epoch: 6 step: 680 loss: 0.057410676032304764\n",
            "epoch: 6 step: 690 loss: 0.052334222942590714\n",
            "epoch: 6 step: 700 loss: 0.054001182317733765\n",
            "epoch: 6 step: 710 loss: 0.04984358325600624\n",
            "epoch: 6 step: 720 loss: 0.040935397148132324\n",
            "epoch: 6 step: 730 loss: 0.046278223395347595\n",
            "epoch: 6 step: 740 loss: 0.05681362748146057\n",
            "epoch: 6 step: 750 loss: 0.04774198681116104\n",
            "epoch: 6 step: 760 loss: 0.0641188770532608\n",
            "epoch: 6 step: 770 loss: 0.05925391614437103\n",
            "epoch: 6 step: 780 loss: 0.04478263854980469\n",
            "epoch: 6 step: 790 loss: 0.055961910635232925\n",
            "epoch: 6 step: 800 loss: 0.06221647560596466\n",
            "epoch: 6 step: 810 loss: 0.056963447481393814\n",
            "epoch: 6 step: 820 loss: 0.037925634533166885\n",
            "epoch: 6 step: 830 loss: 0.04818496108055115\n",
            "epoch: 6 step: 840 loss: 0.056577734649181366\n",
            "epoch: 6 step: 850 loss: 0.05797607824206352\n",
            "epoch: 6 step: 860 loss: 0.034893374890089035\n",
            "epoch: 6 step: 870 loss: 0.0641346275806427\n",
            "epoch: 6 step: 880 loss: 0.050428032875061035\n",
            "epoch: 6 step: 890 loss: 0.05082680284976959\n",
            "epoch: 6 step: 900 loss: 0.03776194155216217\n",
            "epoch: 6 step: 910 loss: 0.05150787532329559\n",
            "epoch: 6 step: 920 loss: 0.044289954006671906\n",
            "epoch: 6 step: 930 loss: 0.0485028475522995\n",
            "epoch: 6 step: 940 loss: 0.0357833206653595\n",
            "epoch: 6 step: 950 loss: 0.04990794509649277\n",
            "epoch: 6 step: 960 loss: 0.05105163902044296\n",
            "epoch: 6 step: 970 loss: 0.05718744173645973\n",
            "epoch: 6 step: 980 loss: 0.060227762907743454\n",
            "epoch: 6 step: 990 loss: 0.06088322028517723\n",
            "epoch: 6 step: 1000 loss: 0.047003112733364105\n",
            "epoch: 6 step: 1010 loss: 0.058302365243434906\n",
            "epoch: 6 step: 1020 loss: 0.057131268084049225\n",
            "epoch: 6 step: 1030 loss: 0.06475034356117249\n",
            "epoch: 6 step: 1040 loss: 0.05672265961766243\n",
            "epoch: 6 step: 1050 loss: 0.050114333629608154\n",
            "epoch: 6 step: 1060 loss: 0.046535782516002655\n",
            "epoch: 6 step: 1070 loss: 0.06066770851612091\n",
            "epoch: 6 step: 1080 loss: 0.044975146651268005\n",
            "epoch: 6 step: 1090 loss: 0.03891410678625107\n",
            "epoch: 6 step: 1100 loss: 0.04965817928314209\n",
            "epoch: 6 step: 1110 loss: 0.04217350482940674\n",
            "epoch: 6 step: 1120 loss: 0.050259023904800415\n",
            "epoch: 6 step: 1130 loss: 0.05158449709415436\n",
            "epoch: 6 step: 1140 loss: 0.042802754789590836\n",
            "epoch: 6 step: 1150 loss: 0.0577412024140358\n",
            "epoch: 6 step: 1160 loss: 0.04088588058948517\n",
            "epoch: 6 step: 1170 loss: 0.04417430981993675\n",
            "epoch: 6 step: 1180 loss: 0.04249070584774017\n",
            "epoch: 6 step: 1190 loss: 0.054244160652160645\n",
            "epoch: 6 step: 1200 loss: 0.040781985968351364\n",
            "epoch: 6 step: 1210 loss: 0.04701392352581024\n",
            "epoch: 6 step: 1220 loss: 0.048372264951467514\n",
            "epoch: 6 step: 1230 loss: 0.0604107528924942\n",
            "epoch: 6 step: 1240 loss: 0.042281623929739\n",
            "epoch: 6 step: 1250 loss: 0.040536731481552124\n",
            "epoch: 7 step: 10 loss: 0.05455758795142174\n",
            "epoch: 7 step: 20 loss: 0.04397979751229286\n",
            "epoch: 7 step: 30 loss: 0.04660118371248245\n",
            "epoch: 7 step: 40 loss: 0.045872002840042114\n",
            "epoch: 7 step: 50 loss: 0.05613318830728531\n",
            "epoch: 7 step: 60 loss: 0.04648091271519661\n",
            "epoch: 7 step: 70 loss: 0.035147085785865784\n",
            "epoch: 7 step: 80 loss: 0.046333760023117065\n",
            "epoch: 7 step: 90 loss: 0.05384083092212677\n",
            "epoch: 7 step: 100 loss: 0.04912734776735306\n",
            "epoch: 7 step: 110 loss: 0.03491542488336563\n",
            "epoch: 7 step: 120 loss: 0.06277436763048172\n",
            "epoch: 7 step: 130 loss: 0.06232217699289322\n",
            "epoch: 7 step: 140 loss: 0.058681949973106384\n",
            "epoch: 7 step: 150 loss: 0.04416132718324661\n",
            "epoch: 7 step: 160 loss: 0.043813448399305344\n",
            "epoch: 7 step: 170 loss: 0.03191938251256943\n",
            "epoch: 7 step: 180 loss: 0.03975601866841316\n",
            "epoch: 7 step: 190 loss: 0.03878749907016754\n",
            "epoch: 7 step: 200 loss: 0.044322334229946136\n",
            "epoch: 7 step: 210 loss: 0.04931478947401047\n",
            "epoch: 7 step: 220 loss: 0.042825035750865936\n",
            "epoch: 7 step: 230 loss: 0.05260250344872475\n",
            "epoch: 7 step: 240 loss: 0.041834548115730286\n",
            "epoch: 7 step: 250 loss: 0.04520796239376068\n",
            "epoch: 7 step: 260 loss: 0.04519713670015335\n",
            "epoch: 7 step: 270 loss: 0.04079006612300873\n",
            "epoch: 7 step: 280 loss: 0.04128270596265793\n",
            "epoch: 7 step: 290 loss: 0.05710628256201744\n",
            "epoch: 7 step: 300 loss: 0.042198777198791504\n",
            "epoch: 7 step: 310 loss: 0.04770079255104065\n",
            "epoch: 7 step: 320 loss: 0.04677392542362213\n",
            "epoch: 7 step: 330 loss: 0.05307713896036148\n",
            "epoch: 7 step: 340 loss: 0.040374815464019775\n",
            "epoch: 7 step: 350 loss: 0.052034832537174225\n",
            "epoch: 7 step: 360 loss: 0.031000446528196335\n",
            "epoch: 7 step: 370 loss: 0.056060612201690674\n",
            "epoch: 7 step: 380 loss: 0.05384533107280731\n",
            "epoch: 7 step: 390 loss: 0.05263662338256836\n",
            "epoch: 7 step: 400 loss: 0.06612040847539902\n",
            "epoch: 7 step: 410 loss: 0.04227945953607559\n",
            "epoch: 7 step: 420 loss: 0.051020100712776184\n",
            "epoch: 7 step: 430 loss: 0.04292092099785805\n",
            "epoch: 7 step: 440 loss: 0.048900388181209564\n",
            "epoch: 7 step: 450 loss: 0.048063725233078\n",
            "epoch: 7 step: 460 loss: 0.04302602633833885\n",
            "epoch: 7 step: 470 loss: 0.045536670833826065\n",
            "epoch: 7 step: 480 loss: 0.05996256321668625\n",
            "epoch: 7 step: 490 loss: 0.0326528400182724\n",
            "epoch: 7 step: 500 loss: 0.04807484894990921\n",
            "epoch: 7 step: 510 loss: 0.03444784879684448\n",
            "epoch: 7 step: 520 loss: 0.04727208614349365\n",
            "epoch: 7 step: 530 loss: 0.03619619831442833\n",
            "epoch: 7 step: 540 loss: 0.04273936524987221\n",
            "epoch: 7 step: 550 loss: 0.02986617013812065\n",
            "epoch: 7 step: 560 loss: 0.04880528897047043\n",
            "epoch: 7 step: 570 loss: 0.03608396276831627\n",
            "epoch: 7 step: 580 loss: 0.03735271841287613\n",
            "epoch: 7 step: 590 loss: 0.03331810235977173\n",
            "epoch: 7 step: 600 loss: 0.030556082725524902\n",
            "epoch: 7 step: 610 loss: 0.04658354073762894\n",
            "epoch: 7 step: 620 loss: 0.04797537252306938\n",
            "epoch: 7 step: 630 loss: 0.06362743675708771\n",
            "epoch: 7 step: 640 loss: 0.03841530531644821\n",
            "epoch: 7 step: 650 loss: 0.043371278792619705\n",
            "epoch: 7 step: 660 loss: 0.04491933062672615\n",
            "epoch: 7 step: 670 loss: 0.037610266357660294\n",
            "epoch: 7 step: 680 loss: 0.04558754712343216\n",
            "epoch: 7 step: 690 loss: 0.04905064031481743\n",
            "epoch: 7 step: 700 loss: 0.04687138646841049\n",
            "epoch: 7 step: 710 loss: 0.04213531315326691\n",
            "epoch: 7 step: 720 loss: 0.03434465080499649\n",
            "epoch: 7 step: 730 loss: 0.04046996682882309\n",
            "epoch: 7 step: 740 loss: 0.05258268862962723\n",
            "epoch: 7 step: 750 loss: 0.04259532690048218\n",
            "epoch: 7 step: 760 loss: 0.05539655685424805\n",
            "epoch: 7 step: 770 loss: 0.05163324624300003\n",
            "epoch: 7 step: 780 loss: 0.03733063489198685\n",
            "epoch: 7 step: 790 loss: 0.04712597653269768\n",
            "epoch: 7 step: 800 loss: 0.05532671511173248\n",
            "epoch: 7 step: 810 loss: 0.053341858088970184\n",
            "epoch: 7 step: 820 loss: 0.029542405158281326\n",
            "epoch: 7 step: 830 loss: 0.0430382639169693\n",
            "epoch: 7 step: 840 loss: 0.04703552648425102\n",
            "epoch: 7 step: 850 loss: 0.050492413341999054\n",
            "epoch: 7 step: 860 loss: 0.027753282338380814\n",
            "epoch: 7 step: 870 loss: 0.058980248868465424\n",
            "epoch: 7 step: 880 loss: 0.04511161148548126\n",
            "epoch: 7 step: 890 loss: 0.04530845955014229\n",
            "epoch: 7 step: 900 loss: 0.026969220489263535\n",
            "epoch: 7 step: 910 loss: 0.04604130983352661\n",
            "epoch: 7 step: 920 loss: 0.037726908922195435\n",
            "epoch: 7 step: 930 loss: 0.04228508472442627\n",
            "epoch: 7 step: 940 loss: 0.034110553562641144\n",
            "epoch: 7 step: 950 loss: 0.044606443494558334\n",
            "epoch: 7 step: 960 loss: 0.045907847583293915\n",
            "epoch: 7 step: 970 loss: 0.05226460099220276\n",
            "epoch: 7 step: 980 loss: 0.05555647611618042\n",
            "epoch: 7 step: 990 loss: 0.05356355756521225\n",
            "epoch: 7 step: 1000 loss: 0.04037678986787796\n",
            "epoch: 7 step: 1010 loss: 0.05179633945226669\n",
            "epoch: 7 step: 1020 loss: 0.049984268844127655\n",
            "epoch: 7 step: 1030 loss: 0.05397232994437218\n",
            "epoch: 7 step: 1040 loss: 0.04972292482852936\n",
            "epoch: 7 step: 1050 loss: 0.04266755282878876\n",
            "epoch: 7 step: 1060 loss: 0.04045550897717476\n",
            "epoch: 7 step: 1070 loss: 0.054593056440353394\n",
            "epoch: 7 step: 1080 loss: 0.04188299551606178\n",
            "epoch: 7 step: 1090 loss: 0.03323942422866821\n",
            "epoch: 7 step: 1100 loss: 0.04317618906497955\n",
            "epoch: 7 step: 1110 loss: 0.03393062949180603\n",
            "epoch: 7 step: 1120 loss: 0.04064435511827469\n",
            "epoch: 7 step: 1130 loss: 0.04523032158613205\n",
            "epoch: 7 step: 1140 loss: 0.03587997704744339\n",
            "epoch: 7 step: 1150 loss: 0.05023445188999176\n",
            "epoch: 7 step: 1160 loss: 0.02991759218275547\n",
            "epoch: 7 step: 1170 loss: 0.03476127237081528\n",
            "epoch: 7 step: 1180 loss: 0.0370669811964035\n",
            "epoch: 7 step: 1190 loss: 0.04496433213353157\n",
            "epoch: 7 step: 1200 loss: 0.0348195917904377\n",
            "epoch: 7 step: 1210 loss: 0.041327208280563354\n",
            "epoch: 7 step: 1220 loss: 0.041606612503528595\n",
            "epoch: 7 step: 1230 loss: 0.05317218601703644\n",
            "epoch: 7 step: 1240 loss: 0.035215482115745544\n",
            "epoch: 7 step: 1250 loss: 0.037385158240795135\n",
            "epoch: 8 step: 10 loss: 0.04714043065905571\n",
            "epoch: 8 step: 20 loss: 0.036610838025808334\n",
            "epoch: 8 step: 30 loss: 0.04238136112689972\n",
            "epoch: 8 step: 40 loss: 0.039495185017585754\n",
            "epoch: 8 step: 50 loss: 0.04914109408855438\n",
            "epoch: 8 step: 60 loss: 0.03885062038898468\n",
            "epoch: 8 step: 70 loss: 0.02590193971991539\n",
            "epoch: 8 step: 80 loss: 0.04092825949192047\n",
            "epoch: 8 step: 90 loss: 0.04949174448847771\n",
            "epoch: 8 step: 100 loss: 0.042546868324279785\n",
            "epoch: 8 step: 110 loss: 0.02924446016550064\n",
            "epoch: 8 step: 120 loss: 0.054448455572128296\n",
            "epoch: 8 step: 130 loss: 0.05535777285695076\n",
            "epoch: 8 step: 140 loss: 0.048562705516815186\n",
            "epoch: 8 step: 150 loss: 0.0403769351541996\n",
            "epoch: 8 step: 160 loss: 0.03548731654882431\n",
            "epoch: 8 step: 170 loss: 0.02657013013958931\n",
            "epoch: 8 step: 180 loss: 0.033658333122730255\n",
            "epoch: 8 step: 190 loss: 0.03725964576005936\n",
            "epoch: 8 step: 200 loss: 0.03686106950044632\n",
            "epoch: 8 step: 210 loss: 0.04589976370334625\n",
            "epoch: 8 step: 220 loss: 0.03352253884077072\n",
            "epoch: 8 step: 230 loss: 0.04648369923233986\n",
            "epoch: 8 step: 240 loss: 0.03659050166606903\n",
            "epoch: 8 step: 250 loss: 0.04156038910150528\n",
            "epoch: 8 step: 260 loss: 0.04205614700913429\n",
            "epoch: 8 step: 270 loss: 0.03357440233230591\n",
            "epoch: 8 step: 280 loss: 0.03392209857702255\n",
            "epoch: 8 step: 290 loss: 0.0481562577188015\n",
            "epoch: 8 step: 300 loss: 0.032902203500270844\n",
            "epoch: 8 step: 310 loss: 0.04276927933096886\n",
            "epoch: 8 step: 320 loss: 0.036451585590839386\n",
            "epoch: 8 step: 330 loss: 0.045237649232149124\n",
            "epoch: 8 step: 340 loss: 0.036550674587488174\n",
            "epoch: 8 step: 350 loss: 0.04262802377343178\n",
            "epoch: 8 step: 360 loss: 0.022716926410794258\n",
            "epoch: 8 step: 370 loss: 0.047820258885622025\n",
            "epoch: 8 step: 380 loss: 0.0438537523150444\n",
            "epoch: 8 step: 390 loss: 0.04550037533044815\n",
            "epoch: 8 step: 400 loss: 0.058464497327804565\n",
            "epoch: 8 step: 410 loss: 0.03942492976784706\n",
            "epoch: 8 step: 420 loss: 0.04108474403619766\n",
            "epoch: 8 step: 430 loss: 0.03364936634898186\n",
            "epoch: 8 step: 440 loss: 0.037862181663513184\n",
            "epoch: 8 step: 450 loss: 0.04300859570503235\n",
            "epoch: 8 step: 460 loss: 0.03990452364087105\n",
            "epoch: 8 step: 470 loss: 0.04171086102724075\n",
            "epoch: 8 step: 480 loss: 0.055143263190984726\n",
            "epoch: 8 step: 490 loss: 0.02413095161318779\n",
            "epoch: 8 step: 500 loss: 0.03901028260588646\n",
            "epoch: 8 step: 510 loss: 0.030508320778608322\n",
            "epoch: 8 step: 520 loss: 0.03891592472791672\n",
            "epoch: 8 step: 530 loss: 0.02952120266854763\n",
            "epoch: 8 step: 540 loss: 0.037752024829387665\n",
            "epoch: 8 step: 550 loss: 0.024199914187192917\n",
            "epoch: 8 step: 560 loss: 0.03947073966264725\n",
            "epoch: 8 step: 570 loss: 0.02867898903787136\n",
            "epoch: 8 step: 580 loss: 0.03177833557128906\n",
            "epoch: 8 step: 590 loss: 0.030824018642306328\n",
            "epoch: 8 step: 600 loss: 0.02471053972840309\n",
            "epoch: 8 step: 610 loss: 0.03973977267742157\n",
            "epoch: 8 step: 620 loss: 0.03894629329442978\n",
            "epoch: 8 step: 630 loss: 0.05703660100698471\n",
            "epoch: 8 step: 640 loss: 0.034240853041410446\n",
            "epoch: 8 step: 650 loss: 0.03477004915475845\n",
            "epoch: 8 step: 660 loss: 0.035316742956638336\n",
            "epoch: 8 step: 670 loss: 0.030777085572481155\n",
            "epoch: 8 step: 680 loss: 0.035892002284526825\n",
            "epoch: 8 step: 690 loss: 0.040405143052339554\n",
            "epoch: 8 step: 700 loss: 0.04167187213897705\n",
            "epoch: 8 step: 710 loss: 0.03729794919490814\n",
            "epoch: 8 step: 720 loss: 0.024861251935362816\n",
            "epoch: 8 step: 730 loss: 0.03476724028587341\n",
            "epoch: 8 step: 740 loss: 0.043325863778591156\n",
            "epoch: 8 step: 750 loss: 0.040387727320194244\n",
            "epoch: 8 step: 760 loss: 0.04755009710788727\n",
            "epoch: 8 step: 770 loss: 0.04310161620378494\n",
            "epoch: 8 step: 780 loss: 0.028226327151060104\n",
            "epoch: 8 step: 790 loss: 0.04089783877134323\n",
            "epoch: 8 step: 800 loss: 0.04697951674461365\n",
            "epoch: 8 step: 810 loss: 0.04778391495347023\n",
            "epoch: 8 step: 820 loss: 0.02353287860751152\n",
            "epoch: 8 step: 830 loss: 0.034929320216178894\n",
            "epoch: 8 step: 840 loss: 0.03989104554057121\n",
            "epoch: 8 step: 850 loss: 0.04614018648862839\n",
            "epoch: 8 step: 860 loss: 0.022009894251823425\n",
            "epoch: 8 step: 870 loss: 0.055863358080387115\n",
            "epoch: 8 step: 880 loss: 0.038195982575416565\n",
            "epoch: 8 step: 890 loss: 0.03905513882637024\n",
            "epoch: 8 step: 900 loss: 0.021022174507379532\n",
            "epoch: 8 step: 910 loss: 0.04391150176525116\n",
            "epoch: 8 step: 920 loss: 0.03183671087026596\n",
            "epoch: 8 step: 930 loss: 0.0367097333073616\n",
            "epoch: 8 step: 940 loss: 0.028496410697698593\n",
            "epoch: 8 step: 950 loss: 0.038617461919784546\n",
            "epoch: 8 step: 960 loss: 0.040632687509059906\n",
            "epoch: 8 step: 970 loss: 0.047078438103199005\n",
            "epoch: 8 step: 980 loss: 0.05030849203467369\n",
            "epoch: 8 step: 990 loss: 0.046293240040540695\n",
            "epoch: 8 step: 1000 loss: 0.030333220958709717\n",
            "epoch: 8 step: 1010 loss: 0.04613889381289482\n",
            "epoch: 8 step: 1020 loss: 0.045949630439281464\n",
            "epoch: 8 step: 1030 loss: 0.05057024955749512\n",
            "epoch: 8 step: 1040 loss: 0.04368040710687637\n",
            "epoch: 8 step: 1050 loss: 0.03720006346702576\n",
            "epoch: 8 step: 1060 loss: 0.03558052331209183\n",
            "epoch: 8 step: 1070 loss: 0.04537281021475792\n",
            "epoch: 8 step: 1080 loss: 0.03626369684934616\n",
            "epoch: 8 step: 1090 loss: 0.026775484904646873\n",
            "epoch: 8 step: 1100 loss: 0.042285770177841187\n",
            "epoch: 8 step: 1110 loss: 0.02837127074599266\n",
            "epoch: 8 step: 1120 loss: 0.03574452921748161\n",
            "epoch: 8 step: 1130 loss: 0.03948284313082695\n",
            "epoch: 8 step: 1140 loss: 0.03192643076181412\n",
            "epoch: 8 step: 1150 loss: 0.05186307057738304\n",
            "epoch: 8 step: 1160 loss: 0.025758283212780952\n",
            "epoch: 8 step: 1170 loss: 0.02965501882135868\n",
            "epoch: 8 step: 1180 loss: 0.03156386315822601\n",
            "epoch: 8 step: 1190 loss: 0.03367134556174278\n",
            "epoch: 8 step: 1200 loss: 0.029031168669462204\n",
            "epoch: 8 step: 1210 loss: 0.03392120823264122\n",
            "epoch: 8 step: 1220 loss: 0.03529225289821625\n",
            "epoch: 8 step: 1230 loss: 0.045717753469944\n",
            "epoch: 8 step: 1240 loss: 0.026254616677761078\n",
            "epoch: 8 step: 1250 loss: 0.028970466926693916\n",
            "epoch: 9 step: 10 loss: 0.03824692219495773\n",
            "epoch: 9 step: 20 loss: 0.031900350004434586\n",
            "epoch: 9 step: 30 loss: 0.032933399081230164\n",
            "epoch: 9 step: 40 loss: 0.035396937280893326\n",
            "epoch: 9 step: 50 loss: 0.03697318583726883\n",
            "epoch: 9 step: 60 loss: 0.030951987951993942\n",
            "epoch: 9 step: 70 loss: 0.020980756729841232\n",
            "epoch: 9 step: 80 loss: 0.031354449689388275\n",
            "epoch: 9 step: 90 loss: 0.04109961539506912\n",
            "epoch: 9 step: 100 loss: 0.03587103635072708\n",
            "epoch: 9 step: 110 loss: 0.024085909128189087\n",
            "epoch: 9 step: 120 loss: 0.04845822975039482\n",
            "epoch: 9 step: 130 loss: 0.0410926453769207\n",
            "epoch: 9 step: 140 loss: 0.04492921754717827\n",
            "epoch: 9 step: 150 loss: 0.03680184856057167\n",
            "epoch: 9 step: 160 loss: 0.032163772732019424\n",
            "epoch: 9 step: 170 loss: 0.021655768156051636\n",
            "epoch: 9 step: 180 loss: 0.025756960734725\n",
            "epoch: 9 step: 190 loss: 0.027256939560174942\n",
            "epoch: 9 step: 200 loss: 0.028895694762468338\n",
            "epoch: 9 step: 210 loss: 0.03714696317911148\n",
            "epoch: 9 step: 220 loss: 0.028645476326346397\n",
            "epoch: 9 step: 230 loss: 0.04100114852190018\n",
            "epoch: 9 step: 240 loss: 0.025975773110985756\n",
            "epoch: 9 step: 250 loss: 0.033390291035175323\n",
            "epoch: 9 step: 260 loss: 0.033956125378608704\n",
            "epoch: 9 step: 270 loss: 0.026203583925962448\n",
            "epoch: 9 step: 280 loss: 0.02675459533929825\n",
            "epoch: 9 step: 290 loss: 0.03995156288146973\n",
            "epoch: 9 step: 300 loss: 0.02988589182496071\n",
            "epoch: 9 step: 310 loss: 0.03439192846417427\n",
            "epoch: 9 step: 320 loss: 0.03333060070872307\n",
            "epoch: 9 step: 330 loss: 0.03971259668469429\n",
            "epoch: 9 step: 340 loss: 0.03501514345407486\n",
            "epoch: 9 step: 350 loss: 0.035691652446985245\n",
            "epoch: 9 step: 360 loss: 0.01946876384317875\n",
            "epoch: 9 step: 370 loss: 0.044153809547424316\n",
            "epoch: 9 step: 380 loss: 0.04257486015558243\n",
            "epoch: 9 step: 390 loss: 0.038894180208444595\n",
            "epoch: 9 step: 400 loss: 0.04943813383579254\n",
            "epoch: 9 step: 410 loss: 0.03389204666018486\n",
            "epoch: 9 step: 420 loss: 0.0349784716963768\n",
            "epoch: 9 step: 430 loss: 0.030278101563453674\n",
            "epoch: 9 step: 440 loss: 0.03430834412574768\n",
            "epoch: 9 step: 450 loss: 0.03682311251759529\n",
            "epoch: 9 step: 460 loss: 0.03266412392258644\n",
            "epoch: 9 step: 470 loss: 0.029815733432769775\n",
            "epoch: 9 step: 480 loss: 0.04582832753658295\n",
            "epoch: 9 step: 490 loss: 0.02098403126001358\n",
            "epoch: 9 step: 500 loss: 0.03657791018486023\n",
            "epoch: 9 step: 510 loss: 0.023233868181705475\n",
            "epoch: 9 step: 520 loss: 0.032732315361499786\n",
            "epoch: 9 step: 530 loss: 0.023347923532128334\n",
            "epoch: 9 step: 540 loss: 0.02821275219321251\n",
            "epoch: 9 step: 550 loss: 0.01854490116238594\n",
            "epoch: 9 step: 560 loss: 0.03507262468338013\n",
            "epoch: 9 step: 570 loss: 0.02207472175359726\n",
            "epoch: 9 step: 580 loss: 0.023770464584231377\n",
            "epoch: 9 step: 590 loss: 0.03176677227020264\n",
            "epoch: 9 step: 600 loss: 0.020059306174516678\n",
            "epoch: 9 step: 610 loss: 0.03534621745347977\n",
            "epoch: 9 step: 620 loss: 0.03592287376523018\n",
            "epoch: 9 step: 630 loss: 0.05132480710744858\n",
            "epoch: 9 step: 640 loss: 0.029228106141090393\n",
            "epoch: 9 step: 650 loss: 0.030213791877031326\n",
            "epoch: 9 step: 660 loss: 0.02922063320875168\n",
            "epoch: 9 step: 670 loss: 0.0295255109667778\n",
            "epoch: 9 step: 680 loss: 0.03074703924357891\n",
            "epoch: 9 step: 690 loss: 0.03449999541044235\n",
            "epoch: 9 step: 700 loss: 0.030879009515047073\n",
            "epoch: 9 step: 710 loss: 0.02952849119901657\n",
            "epoch: 9 step: 720 loss: 0.018591562286019325\n",
            "epoch: 9 step: 730 loss: 0.02841043844819069\n",
            "epoch: 9 step: 740 loss: 0.030167272314429283\n",
            "epoch: 9 step: 750 loss: 0.028846953064203262\n",
            "epoch: 9 step: 760 loss: 0.043696001172065735\n",
            "epoch: 9 step: 770 loss: 0.03935452923178673\n",
            "epoch: 9 step: 780 loss: 0.022467296570539474\n",
            "epoch: 9 step: 790 loss: 0.034269459545612335\n",
            "epoch: 9 step: 800 loss: 0.04425423964858055\n",
            "epoch: 9 step: 810 loss: 0.03612102195620537\n",
            "epoch: 9 step: 820 loss: 0.01793130487203598\n",
            "epoch: 9 step: 830 loss: 0.030576329678297043\n",
            "epoch: 9 step: 840 loss: 0.036292970180511475\n",
            "epoch: 9 step: 850 loss: 0.037026602774858475\n",
            "epoch: 9 step: 860 loss: 0.017417464405298233\n",
            "epoch: 9 step: 870 loss: 0.04820897802710533\n",
            "epoch: 9 step: 880 loss: 0.03257041424512863\n",
            "epoch: 9 step: 890 loss: 0.03505890443921089\n",
            "epoch: 9 step: 900 loss: 0.01696304976940155\n",
            "epoch: 9 step: 910 loss: 0.033861927688121796\n",
            "epoch: 9 step: 920 loss: 0.026249082759022713\n",
            "epoch: 9 step: 930 loss: 0.03296324983239174\n",
            "epoch: 9 step: 940 loss: 0.028873257339000702\n",
            "epoch: 9 step: 950 loss: 0.031033389270305634\n",
            "epoch: 9 step: 960 loss: 0.0334695465862751\n",
            "epoch: 9 step: 970 loss: 0.03663717210292816\n",
            "epoch: 9 step: 980 loss: 0.0420234389603138\n",
            "epoch: 9 step: 990 loss: 0.03914221003651619\n",
            "epoch: 9 step: 1000 loss: 0.026052655652165413\n",
            "epoch: 9 step: 1010 loss: 0.03842952102422714\n",
            "epoch: 9 step: 1020 loss: 0.03439248725771904\n",
            "epoch: 9 step: 1030 loss: 0.045832354575395584\n",
            "epoch: 9 step: 1040 loss: 0.0347922258079052\n",
            "epoch: 9 step: 1050 loss: 0.03322106599807739\n",
            "epoch: 9 step: 1060 loss: 0.02841911092400551\n",
            "epoch: 9 step: 1070 loss: 0.039302896708250046\n",
            "epoch: 9 step: 1080 loss: 0.028118252754211426\n",
            "epoch: 9 step: 1090 loss: 0.02001495286822319\n",
            "epoch: 9 step: 1100 loss: 0.027470959350466728\n",
            "epoch: 9 step: 1110 loss: 0.022743212059140205\n",
            "epoch: 9 step: 1120 loss: 0.02511327713727951\n",
            "epoch: 9 step: 1130 loss: 0.03505215793848038\n",
            "epoch: 9 step: 1140 loss: 0.025231197476387024\n",
            "epoch: 9 step: 1150 loss: 0.03273862227797508\n",
            "epoch: 9 step: 1160 loss: 0.016591191291809082\n",
            "epoch: 9 step: 1170 loss: 0.02686941996216774\n",
            "epoch: 9 step: 1180 loss: 0.024420563131570816\n",
            "epoch: 9 step: 1190 loss: 0.02424084022641182\n",
            "epoch: 9 step: 1200 loss: 0.024631062522530556\n",
            "epoch: 9 step: 1210 loss: 0.03013782761991024\n",
            "epoch: 9 step: 1220 loss: 0.02711782231926918\n",
            "epoch: 9 step: 1230 loss: 0.03399115055799484\n",
            "epoch: 9 step: 1240 loss: 0.021620556712150574\n",
            "epoch: 9 step: 1250 loss: 0.020496321842074394\n",
            "epoch: 10 step: 10 loss: 0.02816716581583023\n",
            "epoch: 10 step: 20 loss: 0.025930583477020264\n",
            "epoch: 10 step: 30 loss: 0.022768190130591393\n",
            "epoch: 10 step: 40 loss: 0.02848537638783455\n",
            "epoch: 10 step: 50 loss: 0.030158761888742447\n",
            "epoch: 10 step: 60 loss: 0.026451442390680313\n",
            "epoch: 10 step: 70 loss: 0.01942446082830429\n",
            "epoch: 10 step: 80 loss: 0.029215406626462936\n",
            "epoch: 10 step: 90 loss: 0.0342075414955616\n",
            "epoch: 10 step: 100 loss: 0.029293982312083244\n",
            "epoch: 10 step: 110 loss: 0.01958879455924034\n",
            "epoch: 10 step: 120 loss: 0.04107194021344185\n",
            "epoch: 10 step: 130 loss: 0.03653593361377716\n",
            "epoch: 10 step: 140 loss: 0.03608597069978714\n",
            "epoch: 10 step: 150 loss: 0.02944956161081791\n",
            "epoch: 10 step: 160 loss: 0.0251584742218256\n",
            "epoch: 10 step: 170 loss: 0.016866853460669518\n",
            "epoch: 10 step: 180 loss: 0.022192060947418213\n",
            "epoch: 10 step: 190 loss: 0.024624643847346306\n",
            "epoch: 10 step: 200 loss: 0.023360051214694977\n",
            "epoch: 10 step: 210 loss: 0.03129270672798157\n",
            "epoch: 10 step: 220 loss: 0.02218741551041603\n",
            "epoch: 10 step: 230 loss: 0.037837184965610504\n",
            "epoch: 10 step: 240 loss: 0.020406978204846382\n",
            "epoch: 10 step: 250 loss: 0.02576466277241707\n",
            "epoch: 10 step: 260 loss: 0.02892053872346878\n",
            "epoch: 10 step: 270 loss: 0.025164971128106117\n",
            "epoch: 10 step: 280 loss: 0.019684026017785072\n",
            "epoch: 10 step: 290 loss: 0.035479459911584854\n",
            "epoch: 10 step: 300 loss: 0.025039464235305786\n",
            "epoch: 10 step: 310 loss: 0.026886990293860435\n",
            "epoch: 10 step: 320 loss: 0.022229069843888283\n",
            "epoch: 10 step: 330 loss: 0.0342431478202343\n",
            "epoch: 10 step: 340 loss: 0.02853156253695488\n",
            "epoch: 10 step: 350 loss: 0.03207393363118172\n",
            "epoch: 10 step: 360 loss: 0.016438379883766174\n",
            "epoch: 10 step: 370 loss: 0.03573521599173546\n",
            "epoch: 10 step: 380 loss: 0.03306504338979721\n",
            "epoch: 10 step: 390 loss: 0.028568610548973083\n",
            "epoch: 10 step: 400 loss: 0.04399663954973221\n",
            "epoch: 10 step: 410 loss: 0.024774067103862762\n",
            "epoch: 10 step: 420 loss: 0.028118567541241646\n",
            "epoch: 10 step: 430 loss: 0.023130301386117935\n",
            "epoch: 10 step: 440 loss: 0.024853158742189407\n",
            "epoch: 10 step: 450 loss: 0.027819178998470306\n",
            "epoch: 10 step: 460 loss: 0.029454872012138367\n",
            "epoch: 10 step: 470 loss: 0.028252113610506058\n",
            "epoch: 10 step: 480 loss: 0.03496110439300537\n",
            "epoch: 10 step: 490 loss: 0.016638321802020073\n",
            "epoch: 10 step: 500 loss: 0.03238894045352936\n",
            "epoch: 10 step: 510 loss: 0.017237110063433647\n",
            "epoch: 10 step: 520 loss: 0.026820283383131027\n",
            "epoch: 10 step: 530 loss: 0.01316266693174839\n",
            "epoch: 10 step: 540 loss: 0.02421540953218937\n",
            "epoch: 10 step: 550 loss: 0.017880886793136597\n",
            "epoch: 10 step: 560 loss: 0.032939597964286804\n",
            "epoch: 10 step: 570 loss: 0.018622715026140213\n",
            "epoch: 10 step: 580 loss: 0.014877288602292538\n",
            "epoch: 10 step: 590 loss: 0.017904235050082207\n",
            "epoch: 10 step: 600 loss: 0.016510404646396637\n",
            "epoch: 10 step: 610 loss: 0.026427915319800377\n",
            "epoch: 10 step: 620 loss: 0.03122994303703308\n",
            "epoch: 10 step: 630 loss: 0.04133469983935356\n",
            "epoch: 10 step: 640 loss: 0.02679297886788845\n",
            "epoch: 10 step: 650 loss: 0.029257625341415405\n",
            "epoch: 10 step: 660 loss: 0.020780237391591072\n",
            "epoch: 10 step: 670 loss: 0.0219208262860775\n",
            "epoch: 10 step: 680 loss: 0.021157190203666687\n",
            "epoch: 10 step: 690 loss: 0.027169998735189438\n",
            "epoch: 10 step: 700 loss: 0.02741900272667408\n",
            "epoch: 10 step: 710 loss: 0.02309766784310341\n",
            "epoch: 10 step: 720 loss: 0.0239346232265234\n",
            "epoch: 10 step: 730 loss: 0.027014119550585747\n",
            "epoch: 10 step: 740 loss: 0.024535171687602997\n",
            "epoch: 10 step: 750 loss: 0.02886643260717392\n",
            "epoch: 10 step: 760 loss: 0.03680600970983505\n",
            "epoch: 10 step: 770 loss: 0.028530364856123924\n",
            "epoch: 10 step: 780 loss: 0.02445969730615616\n",
            "epoch: 10 step: 790 loss: 0.03613535687327385\n",
            "epoch: 10 step: 800 loss: 0.03769515082240105\n",
            "epoch: 10 step: 810 loss: 0.027734294533729553\n",
            "epoch: 10 step: 820 loss: 0.013241134583950043\n",
            "epoch: 10 step: 830 loss: 0.027992742136120796\n",
            "epoch: 10 step: 840 loss: 0.03221434727311134\n",
            "epoch: 10 step: 850 loss: 0.031456638127565384\n",
            "epoch: 10 step: 860 loss: 0.008778377436101437\n",
            "epoch: 10 step: 870 loss: 0.039616093039512634\n",
            "epoch: 10 step: 880 loss: 0.024882102385163307\n",
            "epoch: 10 step: 890 loss: 0.026246316730976105\n",
            "epoch: 10 step: 900 loss: 0.013621712103486061\n",
            "epoch: 10 step: 910 loss: 0.03222012519836426\n",
            "epoch: 10 step: 920 loss: 0.027019597589969635\n",
            "epoch: 10 step: 930 loss: 0.02753244712948799\n",
            "epoch: 10 step: 940 loss: 0.020179718732833862\n",
            "epoch: 10 step: 950 loss: 0.022250130772590637\n",
            "epoch: 10 step: 960 loss: 0.026249239221215248\n",
            "epoch: 10 step: 970 loss: 0.032656945288181305\n",
            "epoch: 10 step: 980 loss: 0.03612451255321503\n",
            "epoch: 10 step: 990 loss: 0.03822460025548935\n",
            "epoch: 10 step: 1000 loss: 0.021883336827158928\n",
            "epoch: 10 step: 1010 loss: 0.026993578299880028\n",
            "epoch: 10 step: 1020 loss: 0.030571477487683296\n",
            "epoch: 10 step: 1030 loss: 0.03569002449512482\n",
            "epoch: 10 step: 1040 loss: 0.02858230099081993\n",
            "epoch: 10 step: 1050 loss: 0.026917818933725357\n",
            "epoch: 10 step: 1060 loss: 0.023398445919156075\n",
            "epoch: 10 step: 1070 loss: 0.03131876140832901\n",
            "epoch: 10 step: 1080 loss: 0.023869339376688004\n",
            "epoch: 10 step: 1090 loss: 0.01456585805863142\n",
            "epoch: 10 step: 1100 loss: 0.020177096128463745\n",
            "epoch: 10 step: 1110 loss: 0.016340192407369614\n",
            "epoch: 10 step: 1120 loss: 0.01868189126253128\n",
            "epoch: 10 step: 1130 loss: 0.026631003245711327\n",
            "epoch: 10 step: 1140 loss: 0.016710635274648666\n",
            "epoch: 10 step: 1150 loss: 0.024459298700094223\n",
            "epoch: 10 step: 1160 loss: 0.014947045594453812\n",
            "epoch: 10 step: 1170 loss: 0.01803664118051529\n",
            "epoch: 10 step: 1180 loss: 0.021884096786379814\n",
            "epoch: 10 step: 1190 loss: 0.01938258484005928\n",
            "epoch: 10 step: 1200 loss: 0.01967291347682476\n",
            "epoch: 10 step: 1210 loss: 0.02543632686138153\n",
            "epoch: 10 step: 1220 loss: 0.027637865394353867\n",
            "epoch: 10 step: 1230 loss: 0.027739617973566055\n",
            "epoch: 10 step: 1240 loss: 0.017787329852581024\n",
            "epoch: 10 step: 1250 loss: 0.02368132770061493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_uKOpe8IGJk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "721070ab-808f-4940-8515-ac2f47eaa880"
      },
      "source": [
        "\"\"\"TEST\"\"\"\n",
        "def get_char_count(arg1):\n",
        "    c0 = ALL_CHAR_SET[np.argmax(arg1.cpu().tolist()[0:ALL_CHAR_SET_LEN])]\n",
        "    c1 = ALL_CHAR_SET[np.argmax(arg1.cpu().tolist()[ALL_CHAR_SET_LEN:ALL_CHAR_SET_LEN*2])]\n",
        "    c2 = ALL_CHAR_SET[np.argmax(arg1.cpu().tolist()[ALL_CHAR_SET_LEN*2:ALL_CHAR_SET_LEN*3])]\n",
        "    c3 = ALL_CHAR_SET[np.argmax(arg1.cpu().tolist()[ALL_CHAR_SET_LEN*3:ALL_CHAR_SET_LEN*4])]\n",
        "    c4 = ALL_CHAR_SET[np.argmax(arg1.cpu().tolist()[ALL_CHAR_SET_LEN*4:ALL_CHAR_SET_LEN*5])]\n",
        "    c5 = ALL_CHAR_SET[np.argmax(arg1.cpu().tolist()[ALL_CHAR_SET_LEN*5:ALL_CHAR_SET_LEN*6])]\n",
        "    c6 = ALL_CHAR_SET[np.argmax(arg1.cpu().tolist()[ALL_CHAR_SET_LEN*6:ALL_CHAR_SET_LEN*7])]\n",
        "    return c0, c1, c2,c3, c4, c5, c6 \n",
        " \n",
        "\n",
        "\n",
        "char_correct = 0\n",
        "word_correct = 0\n",
        "total = 0\n",
        "\n",
        "betternet.eval()\n",
        "lstm1.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for step, (img, label_oh, label) in enumerate(test_dl):\n",
        "        char_count =0\n",
        "        img = Variable(img).cuda()\n",
        "        label_oh = Variable(label_oh.float()).cuda()\n",
        "        #pred, feature = betternet(img)\n",
        "        pred= betternet(img)\n",
        "\n",
        "        #label_len = label[0]\n",
        "        #pred = pred.squeeze(0)\n",
        "        #label_oh = label_oh.squeeze(0)\n",
        "        \n",
        "        c0,c1,c2,c3,c4,c5,c6 = get_char_count(pred.squeeze()) \n",
        "        d0,d1,d2,d3,d4,d5,d6 = get_char_count(label_oh[0]) \n",
        "         \n",
        "        c = '%s%s%s%s%s%s%s' % (c0, c1, c2, c3, c4, c5, c6)\n",
        "        d = '%s%s%s%s%s%s%s' % (d0, d1, d2, d3, d4, d5, d6)\n",
        "        #print(c)\n",
        "        #print(d)\n",
        "        #print(\" \")\n",
        "    \n",
        "        char_count += (c0==d0)+(c1==d1)+(c2==d2)+(c3==d3)+(c4==d4)+(c5==d5)+(c6==d6)\n",
        "        char_correct += char_count\n",
        "\n",
        "        if(bool(str(label[0]) in str(c))):\n",
        "            word_correct+=1\n",
        "\n",
        "        total += 1\n",
        "       \n",
        "print(100/7*char_correct/total)\n",
        "print(100*word_correct/total)\n",
        "\"\"\"END TEST\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-129-efd968c570dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mlabel_oh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_oh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#pred, feature = betternet(img)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbetternet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m#label_len = label[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-126-516917d084fe>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-126-516917d084fe>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0my3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0my4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBetterNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1921\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1922\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m     )\n\u001b[1;32m   1925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQnvpkZsKYJA",
        "colab_type": "text"
      },
      "source": [
        "# 새 섹션"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_IIGnHhKYo5",
        "colab_type": "text"
      },
      "source": [
        "# 새 섹션"
      ]
    }
  ]
}